{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Temporal CNN Leafy Spurge Generate Training Datasets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM1djQoAeD73w7YDy2klLDC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lake-thomas/spurge-temporal-cnn/blob/main/Temporal_CNN_Leafy_Spurge_Generate_Training_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This is a working Python notebook to implement Google Earth Engine <> TensorFlow for mapping invasive plant species from a time-series of Landsat imagery. In this example, the inputs are invasive species occurrence records from public databases. The model uses 1D-Conv layers in a temporal CNN framework."
      ],
      "metadata": {
        "id": "dYn8af1weyx4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0voGyb2edNx"
      },
      "outputs": [],
      "source": [
        "# Cloud Authentication \n",
        "# Required When Using Default Google Cloud (i.e. Not Using a Hosted VM Runtime Environment)\n",
        "\n",
        "#Connect to hosted VM https://console.cloud.google.com/marketplace/product/colab-marketplace-image-public/colab?project=pacific-engine-346519\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import, authenticate and initialize the Earth Engine library.\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()\n"
      ],
      "metadata": {
        "id": "xnkuanTgeldY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount Google Drive for CSV reading\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Used to export to google cloud\n",
        "from google.cloud import storage\n",
        "import os\n",
        "\n",
        "# Only need this if you're running on GCE\n",
        "#os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'/content/drive/MyDrive/Colab Notebooks/pacific-engine-346519-8368a64310cd.json'\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55pWnkfGq-70",
        "outputId": "3a95e3df-a7a9-4f5c-f5fb-55a9ce171519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ignore Warnings and Errors\n",
        "!pip install geemap\n",
        "import geemap #advanced python function for GEE\n",
        "!pip install geopandas\n",
        "import geopandas #Pandas library to handle geospatial data\n",
        "!pip install fsspec\n",
        "import fsspec # file system specification\n",
        "!pip install gcsfs\n",
        "import gcsfs #google cloud file system"
      ],
      "metadata": {
        "id": "d8Y2tNbvelfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pprint\n",
        "import time\n",
        "from functools import reduce"
      ],
      "metadata": {
        "id": "2rQ6XADyelhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorflow setup.\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZjW4Nibeljy",
        "outputId": "f21a4091-2fe3-454b-9211-fbbb7e86d9b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Functions\n",
        "Functions to process Landsat imagery"
      ],
      "metadata": {
        "id": "wYpLQ3pU_xA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to transfer feature properties to a dictionary.\n",
        "def fc_to_dict(fc):\n",
        "  prop_names = fc.first().propertyNames()\n",
        "  prop_lists = fc.reduceColumns(\n",
        "      reducer=ee.Reducer.toList().repeat(prop_names.size()),\n",
        "      selectors=prop_names).get('list')\n",
        "\n",
        "  return ee.Dictionary.fromLists(prop_names, prop_lists)\n",
        "\n",
        "\n",
        "#Cloud Mask: https://gis.stackexchange.com/questions/274048/apply-cloud-mask-to-landsat-imagery-in-google-earth-engine-python-api\n",
        "def getQABits(image, start, end, mascara): \n",
        "    # Compute the bits we need to extract.\n",
        "    pattern = 0\n",
        "    for i in range(start,end+1):\n",
        "        pattern += 2**i\n",
        "    # Return a single band image of the extracted QA bits, giving the     band a new name.\n",
        "    return image.select([0], [mascara]).bitwiseAnd(pattern).rightShift(start)\n",
        "\n",
        "\n",
        "#Saturated band Mask: https://gis.stackexchange.com/questions/363929/how-to-apply-a-bitmask-for-radiometric-saturation-qa-in-a-image-collection-eart\n",
        "def bitwiseExtract(value, fromBit, toBit):\n",
        "  maskSize = ee.Number(1).add(toBit).subtract(fromBit)\n",
        "  mask = ee.Number(1).leftShift(maskSize).subtract(1)\n",
        "  return value.rightShift(fromBit).bitwiseAnd(mask)\n",
        "\n",
        "\n",
        "#Function to mask out cloudy and saturated pixels and harmonize between Landsat 5/7/8 imagery \n",
        "def maskQuality(image):\n",
        "    # Select the QA band.\n",
        "    QA = image.select('QA_PIXEL')\n",
        "    # Get the internal_cloud_algorithm_flag bit.\n",
        "    sombra = getQABits(QA,3,3,'cloud_shadow')\n",
        "    nubes = getQABits(QA,5,5,'cloud')\n",
        "    #  var cloud_confidence = getQABits(QA,6,7,  'cloud_confidence')\n",
        "    cirrus_detected = getQABits(QA,9,9,'cirrus_detected')\n",
        "    #var cirrus_detected2 = getQABits(QA,8,8,  'cirrus_detected2')\n",
        "    #Return an image masking out cloudy areas.\n",
        "    QA_radsat = image.select('QA_RADSAT')\n",
        "    saturated = bitwiseExtract(QA_radsat, 1, 7)\n",
        "\n",
        "    #Apply the scaling factors to the appropriate bands.\n",
        "    def getFactorImg(factorNames):\n",
        "      factorList = image.toDictionary().select(factorNames).values()\n",
        "      return ee.Image.constant(factorList)\n",
        "\n",
        "    scaleImg = getFactorImg(['REFLECTANCE_MULT_BAND_.|TEMPERATURE_MULT_BAND_ST_B10'])\n",
        "\n",
        "    offsetImg = getFactorImg(['REFLECTANCE_ADD_BAND_.|TEMPERATURE_ADD_BAND_ST_B10'])\n",
        "    \n",
        "    scaled = image.select('SR_B.|ST_B10').multiply(scaleImg).add(offsetImg)\n",
        "\n",
        "    #Replace original bands with scaled bands and apply masks.\n",
        "    return image.addBands(scaled, None, True).updateMask(sombra.eq(0)).updateMask(nubes.eq(0).updateMask(cirrus_detected.eq(0).updateMask(saturated.eq(0))))\n",
        "\n",
        "\n",
        "# Selects and renames bands of interest for Landsat OLI.\n",
        "def renameOli(img):\n",
        "  return img.select(\n",
        "    ['SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7', 'QA_PIXEL', 'QA_RADSAT'],\n",
        "    ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2', 'QA_PIXEL', 'QA_RADSAT'])\n",
        "\n",
        "\n",
        "# Selects and renames bands of interest for TM/ETM+.\n",
        "def renameEtm(img):\n",
        "  return img.select(\n",
        "    ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7', 'QA_PIXEL', 'QA_RADSAT'],\n",
        "    ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2', 'QA_PIXEL', 'QA_RADSAT'])\n",
        "\n",
        "\n",
        "# Adding a NDVI band\n",
        "def addNDVI(image):\n",
        "  ndvi = image.normalizedDifference(['NIR', 'Red']).rename('NDVI')\n",
        "  return image.addBands([ndvi])\n",
        "\n",
        "\n",
        "def mapDates(image):\n",
        "  date = ee.Date(image.get('system:time_start')).format(\"YYYY-MM-dd\")\n",
        "  return image.addBands([date])\n",
        "\n",
        "# Prepares (renames) OLI images.\n",
        "def prepOli(img):\n",
        "  img = renameOli(img)\n",
        "  return img\n",
        "\n",
        "\n",
        "# Prepares (renames) TM/ETM+ images.\n",
        "def prepEtm(img):\n",
        "  orig = img\n",
        "  img = renameEtm(img)\n",
        "  return ee.Image(img.copyProperties(orig, orig.propertyNames()))\n",
        "\n",
        "\n",
        "# Selects and renames bands of interest for TM/ETM+.\n",
        "def renameImageBands_TM(img, year, season):\n",
        "  return img.select(\n",
        "      ['Blue_median', 'Green_median', 'Red_median', 'NIR_median', \n",
        "       'SWIR1_median', 'SWIR2_median', 'QA_PIXEL_median', 'QA_RADSAT_median', 'NDVI_median'],\n",
        "      ['Blue'+str(season)+str(year), 'Green'+str(season)+str(year), 'Red'+str(season)+str(year), 'NIR'+str(season)+str(year),\n",
        "       'SWIR1'+str(season)+str(year), 'SWIR2'+str(season)+str(year), 'QA_PIXEL'+str(season)+str(year), 'QA_RADSAT'+str(season)+str(year), 'NDVI'+str(season)+str(year)])\n",
        "\n",
        "# Selects and renames bands of interest for TM/ETM+.\n",
        "def renameImageBands_ETMOLI(img, year, season):\n",
        "  return img.select(\n",
        "      ['Blue_median_median', 'Green_median_median', 'Red_median_median', 'NIR_median_median', \n",
        "       'SWIR1_median_median', 'SWIR2_median_median', 'QA_PIXEL_median_median', 'QA_RADSAT_median_median', 'NDVI_median_median'],\n",
        "      ['Blue'+str(season)+str(year), 'Green'+str(season)+str(year), 'Red'+str(season)+str(year), 'NIR'+str(season)+str(year),\n",
        "       'SWIR1'+str(season)+str(year), 'SWIR2'+str(season)+str(year), 'QA_PIXEL'+str(season)+str(year), 'QA_RADSAT'+str(season)+str(year), 'NDVI'+str(season)+str(year)])\n",
        "\n",
        "\n",
        "def getLandsatMosaicFromPoints(year, points):\n",
        "  '''\n",
        "  #Time-series extraction developed from\n",
        "  #https://developers.google.com/earth-engine/tutorials/community/time-series-visualization-with-altair#combine_dataframes  \n",
        "\n",
        "  '''\n",
        "\n",
        "  #if Year is between 1985 and 1999 use Landsat 5 TM imagery\n",
        "  if 1985 <= year <= 1999:\n",
        "\n",
        "    tmColMarchApril = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2') \\\n",
        "      .filterDate('{}-03-01'.format(year), '{}-04-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    tmColMarchApril = renameImageBands_TM(tmColMarchApril, year, 'MarchApril')\n",
        "\n",
        "    tmColMayJune = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2') \\\n",
        "      .filterDate('{}-05-01'.format(year), '{}-06-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    tmColMayJune = renameImageBands_TM(tmColMayJune, year, 'MayJune')\n",
        "\n",
        "    tmColJulyAug = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2') \\\n",
        "      .filterDate('{}-07-01'.format(year), '{}-08-31'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    tmColJulyAug = renameImageBands_TM(tmColJulyAug, year, 'JulyAug')\n",
        "\n",
        "    landsat5ImageCol = [tmColMarchApril, tmColMayJune, tmColJulyAug]\n",
        "    return landsat5ImageCol\n",
        "\n",
        "  #if Year is between 2000 and 2012 use mosaic from Landsat 5 TM and Landsat 7 ETM imagery\n",
        "  elif 2000 <= year <= 2012:\n",
        "\n",
        "    etmColMarchApril = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "      .filterDate('{}-03-01'.format(year), '{}-04-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    tmColMarchApril = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2') \\\n",
        "      .filterDate('{}-03-01'.format(year), '{}-04-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    MarchApril = ee.ImageCollection([etmColMarchApril, tmColMarchApril])\n",
        "\n",
        "    etmColMarchApril = MarchApril.reduce('median')\n",
        "\n",
        "    etmColMarchApril = renameImageBands_ETMOLI(etmColMarchApril, year, 'MarchApril')\n",
        "\n",
        "    etmColMayJune = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "      .filterDate('{}-05-01'.format(year), '{}-06-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    tmColMayJune = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2') \\\n",
        "      .filterDate('{}-05-01'.format(year), '{}-06-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    MayJune = ee.ImageCollection([etmColMayJune, tmColMayJune])\n",
        "\n",
        "    etmColMayJune = MayJune.reduce('median')\n",
        "\n",
        "    etmColMayJune = renameImageBands_ETMOLI(etmColMayJune, year, 'MayJune')\n",
        "\n",
        "    etmColJulyAug = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "      .filterDate('{}-07-01'.format(year), '{}-08-31'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    tmColJulyAug = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2') \\\n",
        "      .filterDate('{}-07-01'.format(year), '{}-08-31'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    JulyAug = ee.ImageCollection([etmColJulyAug, tmColJulyAug])\n",
        "\n",
        "    etmColJulyAug = JulyAug.reduce('median')\n",
        "\n",
        "    etmColJulyAug = renameImageBands_ETMOLI(etmColJulyAug, year, 'JulyAug')\n",
        "\n",
        "    landsat5_7ImageCol = [etmColMarchApril, etmColMayJune, etmColJulyAug]\n",
        "    return landsat5_7ImageCol\n",
        "\n",
        "  #if Year is between 2013 and 2020 use mosaic from Landsat 7 ETM and Landsat 8 OLI imagery\n",
        "  elif 2013 <= year <= 2020:\n",
        "\n",
        "    etmColMarchApril = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "      .filterDate('{}-03-01'.format(year), '{}-04-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    oliColMarchApril = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
        "      .filterDate('{}-03-01'.format(year), '{}-04-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepOli) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    MarchApril = ee.ImageCollection([etmColMarchApril, oliColMarchApril])\n",
        "\n",
        "    etmColMarchApril = MarchApril.reduce('median')\n",
        "\n",
        "    etmColMarchApril = renameImageBands_ETMOLI(etmColMarchApril, year, 'MarchApril')\n",
        "\n",
        "    etmColMayJune = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "      .filterDate('{}-05-01'.format(year), '{}-06-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    oliColMayJune = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
        "      .filterDate('{}-05-01'.format(year), '{}-06-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepOli) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    MayJune = ee.ImageCollection([etmColMayJune, oliColMayJune])\n",
        "\n",
        "    etmColMayJune = MayJune.reduce('median')\n",
        "\n",
        "    etmColMayJune = renameImageBands_ETMOLI(etmColMayJune, year, 'MayJune')\n",
        "\n",
        "    etmColJulyAug = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "      .filterDate('{}-07-01'.format(year), '{}-08-31'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median') \\\n",
        "\n",
        "    oliColJulyAug = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
        "      .filterDate('{}-07-01'.format(year), '{}-08-31'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepOli) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    JulyAug = ee.ImageCollection([etmColJulyAug, oliColJulyAug])\n",
        "\n",
        "    etmColJulyAug = JulyAug.reduce('median')\n",
        "\n",
        "    etmColJulyAug = renameImageBands_ETMOLI(etmColJulyAug, year, 'JulyAug')\n",
        "\n",
        "    landsat7_8ImageCol = [etmColMarchApril, etmColMayJune, etmColJulyAug]\n",
        "    return landsat7_8ImageCol\n",
        "\n",
        "\n",
        "\n",
        "def sampleImagestoDataFrame(listofEEImages):\n",
        "    '''\n",
        "    Function takes in a list of three images from a Landsat imagery year (T1, T2, T3)\n",
        "    Returns a merged pandas dataframe of dimensions (rows/samples x bands) ordered from t-1, t, t+1\n",
        "    '''\n",
        "    image1 = listofEEImages[0]\n",
        "    image2 = listofEEImages[1]\n",
        "    image3 = listofEEImages[2]\n",
        "\n",
        "    image1_fc = image1.sampleRegions(collection=newpts, properties=['class'], scale=30)\n",
        "    image2_fc = image2.sampleRegions(collection=newpts, properties=['class'], scale=30)\n",
        "    image3_fc = image3.sampleRegions(collection=newpts, properties=['class'], scale=30)\n",
        "\n",
        "    image1_db_dict = fc_to_dict(image1_fc).getInfo()\n",
        "    image2_db_dict = fc_to_dict(image2_fc).getInfo()\n",
        "    image3_db_dict = fc_to_dict(image3_fc).getInfo()\n",
        "\n",
        "    image1_df = pd.DataFrame(image1_db_dict)\n",
        "    image2_df = pd.DataFrame(image2_db_dict)\n",
        "    image3_df = pd.DataFrame(image3_db_dict)\n",
        "\n",
        "    data_frames = [image1_df, image2_df, image3_df]\n",
        "\n",
        "    df_merged = reduce(lambda left,right: pd.merge(left, right, on='system:index', how='outer'), data_frames).fillna(np.nan)\n",
        "\n",
        "    df_merged_dropna = df_merged.dropna(axis=0, how = 'any')\n",
        "\n",
        "    return df_merged_dropna\n",
        "\n"
      ],
      "metadata": {
        "id": "jzovVIQN_reW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create Bounding Box Moving Windows Across Study Region"
      ],
      "metadata": {
        "id": "UQSsRWGKOUjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Generate Bounding Box Coordinate List for Study Region ###\n",
        "#Starting position of bounding box\n",
        "XY_topLeft = [-116.976099, 48.904682]\n",
        "XY_topRight = [-115.976099, 48.904682]\n",
        "XY_bottomLeft = [-116.976099, 47.904682]\n",
        "XY_bottomRight = [-115.976099, 47.904682]\n",
        "\n",
        "lon_range = 31 #study area spans 31 deg lon\n",
        "lat_range = 12 #study area spans 12 deg lat\n",
        "\n",
        "stepSize = 1 #step by 1 degree of long/latitude\n",
        "\n",
        "\n",
        "def sliding_window(longitude_range, latitude_range, stepSize_box):\n",
        "    lon_list = []\n",
        "    lat_list = []\n",
        "    for lon in range(0, longitude_range, stepSize_box):\n",
        "      for lat in range(0, latitude_range,stepSize_box):\n",
        "        lon_list.append(lon)\n",
        "        lat_list.append(lat)\n",
        "    \n",
        "    return(lon_list, lat_list)\n",
        "\n",
        "def bbox(longitude_range, latitude_range, stepSize_box, topLeft_coord, topRight_coord, bottomLeft_coord, bottomRight_coord):\n",
        "  #Creates a sliding window across the lat/long range\n",
        "  #Returns a list of all lat/long boxes to sample \n",
        "     \n",
        "  lon_list, lat_list = sliding_window(longitude_range, latitude_range, stepSize_box) #Generates two lists: one of longitude[0-31] and one of latitude [0-12] defining study region\n",
        "\n",
        "  #for w in range(len(windows[0])):\n",
        "  #  w_lon = windows[0][w]\n",
        "  #  w_lat = windows[1][w]\n",
        "  #  #print(w_lon, w_lat)\n",
        "\n",
        "  #Top Left Coordinates for BBox\n",
        "  lon_list_X_topLeft = [x + topLeft_coord[0] for x in lon_list]\n",
        "  lat_list_Y_topLeft = [abs(x - topLeft_coord[1]) for x in lat_list]\n",
        "  XY_topLeft_list = list(zip(lon_list_X_topLeft, lat_list_Y_topLeft))\n",
        "\n",
        "  #Bottom Left Coordinates for BBox\n",
        "  lon_list_X_bottomLeft = [x + bottomLeft_coord[0] for x in lon_list]\n",
        "  lat_list_Y_bottomLeft = [abs(x - bottomLeft_coord[1]) for x in lat_list]\n",
        "  XY_bottomLeft_list = list(zip(lon_list_X_bottomLeft, lat_list_Y_bottomLeft))\n",
        "\n",
        "  #Top Right Coordinates for BBox\n",
        "  lon_list_X_topRight = [x + topRight_coord[0] for x in lon_list]\n",
        "  lat_list_Y_topRight = [abs(x - topRight_coord[1]) for x in lat_list]\n",
        "  XY_topRight_list = list(zip(lon_list_X_topRight, lat_list_Y_topRight))\n",
        "\n",
        "  #Bottom Right Coordinates for BBox\n",
        "  lon_list_X_bottomRight = [x + bottomRight_coord[0] for x in lon_list]\n",
        "  lat_list_Y_bottomRight = [abs(x - bottomRight_coord[1]) for x in lat_list]\n",
        "  XY_bottomRight_list = list(zip(lon_list_X_bottomRight, lat_list_Y_bottomRight))\n",
        "\n",
        "  ### Bounding Box Coordinate List\n",
        "  bbox = list(zip(XY_topLeft_list, XY_bottomLeft_list, XY_topRight_list, XY_bottomRight_list))\n",
        "\n",
        "  return bbox\n",
        "\n",
        "\n",
        "bbox_windows = bbox(lon_range, lat_range, stepSize, XY_topLeft, XY_topRight, XY_bottomLeft, XY_bottomRight)\n"
      ],
      "metadata": {
        "id": "g_90d9dDOG4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Workflow to Generate Training Datasets\n",
        "\n",
        "Iteratively generate bounding box arcross study area. Within each bounding box, extract points with labeled land cover values (including leafy spurge) and Landsat imagery. "
      ],
      "metadata": {
        "id": "MZMYPDxe_6ZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training points for leafy spurge & land cover classes (defines extent of landsat imagery)\n",
        "\n",
        "#Load 1m training points sampled from 2019 NLCD and leafy spurge from 2018-2019-2020\n",
        "pts = ee.FeatureCollection('projects/pacific-engine-346519/assets/spurge_landcover_nlcd2019_onemillionpoints_april2022')\n",
        "\n",
        "# Define export for feature class assets\n",
        "outputBucket = 'landcover_samples_nlcd2019_onemillionpoints'\n",
        "# Make sure the bucket exists.\n",
        "print('Found Cloud Storage bucket.' if tf.io.gfile.exists('gs://' + outputBucket) \n",
        "  else 'Output Cloud Storage bucket does not exist.')\n",
        "\n",
        "#define years to sample data (corresponds to satellite image year)\n",
        "years = [2018, 2019, 2020]\n",
        "\n",
        "#Moving Bounding Box Loop to Generate Sample Points\n",
        "for i in range(0, 2):\n",
        "#for i in range(3): testing only\n",
        "  \n",
        "  print(i)\n",
        "\n",
        "  # Define Bounding Box\n",
        "  bbox = bbox_windows[i]\n",
        "  print(bbox)\n",
        "\n",
        "   # Filter points based on AOI\n",
        "  aoi = ee.Geometry.Polygon(bbox)\n",
        "\n",
        "  #Apply Filter\n",
        "  newpts = pts.filterBounds(aoi)\n",
        "  \n",
        "  #How many points?\n",
        "  count = newpts.size() #returns an EE.Number object that we need to convert to an interger\n",
        "  num_points = int(count.getInfo())\n",
        "  print('Number of Points within AOI (Count): ', str(count.getInfo())+'\\n')\n",
        "  \n",
        "  if num_points > 0:\n",
        "\n",
        "    #Sample points given a year, here we want points sampled from three years of Landsat imagery centered on 2019 \n",
        "    LandsatCol_year0 = getLandsatMosaicFromPoints(years[0], newpts) #output is a list of length 3, with three EEimages corresponding to three seasons in a year\n",
        "    LandsatCol_year1 = getLandsatMosaicFromPoints(years[1], newpts)\n",
        "    LandsatCol_year2 = getLandsatMosaicFromPoints(years[2], newpts)\n",
        "\n",
        "    #Convert a LandsatCol_year Image List to a Merged Pandas DataFrame\n",
        "    year0_df = sampleImagestoDataFrame(LandsatCol_year0)\n",
        "    year1_df = sampleImagestoDataFrame(LandsatCol_year1)\n",
        "    year2_df = sampleImagestoDataFrame(LandsatCol_year2)\n",
        "\n",
        "    #Merge DFs and Drop NAs\n",
        "    df_merged = reduce(lambda left,right: pd.merge(left, right, on='system:index', how='outer'), [year0_df, year1_df, year2_df]).fillna(np.nan)\n",
        "\n",
        "    df_merged_dropna = df_merged.dropna(axis=0, how = 'any')\n",
        "\n",
        "    #print(year0_df['class'].value_counts(), year1_df['class'].value_counts(), year2_df['class'].value_counts(), df_merged['class'].value_counts(), df_merged_dropna['class'].value_counts())\n",
        "\n",
        "    #display(df_merged_dropna)\n",
        "\n",
        "\n",
        "    #client = storage.Client()\n",
        "    #bucket = client.get_bucket(outputBucket)\n",
        "      \n",
        "    #bucket.blob('nlcd2019/Landsat_2018-2020_Samples_Test_' + str(i) + '.csv').upload_from_string(df_merged_dropna.to_csv(), 'text/csv')\n",
        "    #df_merged_dropna.to_csv('/content/drive/MyDrive/Colab Notebooks/Temporal CNN Exports CSV/Landsat_2018-2020_Samples_Test_' + str(i) + '.csv')\n",
        "  \n",
        "  \n",
        "  \n"
      ],
      "metadata": {
        "id": "IJyKw53yelmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to Gather Landsat ImageCollection Mosaic from Date/Points Input\n"
      ],
      "metadata": {
        "id": "V4Ou7n3ewbDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Function to Gather Landsat ImageCollection Mosaic from Date/Points Input\n",
        "\n",
        "\n",
        "# Selects and renames bands of interest for TM/ETM+.\n",
        "def renameImageBands(img, year, season):\n",
        "  return img.select(\n",
        "      ['Blue_median', 'Green_median', 'Red_median', 'NIR_median', \n",
        "       'SWIR1_median', 'SWIR2_median', 'QA_PIXEL_median', 'QA_RADSAT_median', 'NDVI_median'],\n",
        "      ['Blue'+str(season)+str(year), 'Green'+str(season)+str(year), 'Red'+str(season)+str(year), 'NIR'+str(season)+str(year),\n",
        "       'SWIR1'+str(season)+str(year), 'SWIR2'+str(season)+str(year), 'QA_PIXEL'+str(season)+str(year), 'QA_RADSAT'+str(season)+str(year), 'NDVI'+str(season)+str(year)])\n",
        "\n",
        "\n",
        "def getLandsatMosaicFromPoints(year, points):\n",
        "  '''\n",
        "  #Time-series extraction developed from\n",
        "  #https://developers.google.com/earth-engine/tutorials/community/time-series-visualization-with-altair#combine_dataframes  \n",
        "\n",
        "  '''\n",
        "\n",
        "  #if Year is between 1985 and 1999 use Landsat 5 TM imagery\n",
        "  if 1985 <= year <= 1999:\n",
        "\n",
        "    tmColMarchApril = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2') \\\n",
        "      .filterDate('{}-03-01'.format(year), '{}-04-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    tmColMarchApril = renameImageBands(tmColMarchApril, year, 'MarchApril')\n",
        "\n",
        "    tmColMayJune = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2') \\\n",
        "      .filterDate('{}-05-01'.format(year), '{}-06-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    tmColMayJune = renameImageBands(tmColMayJune, year, 'MayJune')\n",
        "\n",
        "    tmColJulyAug = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2') \\\n",
        "      .filterDate('{}-07-01'.format(year), '{}-08-31'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    tmColJulyAug = renameImageBands(tmColJulyAug, year, 'JulyAug')\n",
        "\n",
        "    landsat5ImageCol = [tmColMarchApril, tmColMayJune, tmColJulyAug]\n",
        "    return landsat5ImageCol\n",
        "\n",
        "  #if Year is between 2000 and 2012 use mosaic from Landsat 5 TM and Landsat 7 ETM imagery\n",
        "  if 2000 <= year <= 2012:\n",
        "\n",
        "    etmColMarchApril = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "      .filterDate('{}-03-01'.format(year), '{}-04-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    tmColMarchApril = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2') \\\n",
        "      .filterDate('{}-03-01'.format(year), '{}-04-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    MarchApril = ee.ImageCollection([etmColMarchApril, tmColMarchApril])\n",
        "\n",
        "    etmColMarchApril = MarchApril.reduce('median')\n",
        "\n",
        "    etmColMayJune = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "      .filterDate('{}-05-01'.format(year), '{}-06-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    tmColMayJune = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2') \\\n",
        "      .filterDate('{}-05-01'.format(year), '{}-06-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    MayJune = ee.ImageCollection([etmColMayJune, tmColMayJune])\n",
        "\n",
        "    etmColMayJune = MayJune.reduce('median')\n",
        "\n",
        "    etmColJulyAug = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "      .filterDate('{}-07-01'.format(year), '{}-08-31'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    tmColJulyAug = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2') \\\n",
        "      .filterDate('{}-07-01'.format(year), '{}-08-31'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    JulyAug = ee.ImageCollection([etmColJulyAug, tmColJulyAug])\n",
        "\n",
        "    etmColJulyAug = JulyAug.reduce('median')\n",
        "\n",
        "    landsat5_7ImageCol = ee.ImageCollection([etmColMarchApril, etmColMayJune, etmColJulyAug])\n",
        "    return landsat5_7ImageCol\n",
        "\n",
        "  #if Year is between 2013 and 2020 use mosaic from Landsat 7 ETM and Landsat 8 OLI imagery\n",
        "  if 2013 <= year <= 2020:\n",
        "\n",
        "    etmColMarchApril = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "      .filterDate('{}-03-01'.format(year), '{}-04-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    oliColMarchApril = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
        "      .filterDate('{}-03-01'.format(year), '{}-04-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepOli) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    MarchApril = ee.ImageCollection([etmColMarchApril, oliColMarchApril])\n",
        "\n",
        "    etmColMarchApril = MarchApril.reduce('median')\n",
        "\n",
        "    etmColMayJune = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "      .filterDate('{}-05-01'.format(year), '{}-06-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    oliColMayJune = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
        "      .filterDate('{}-05-01'.format(year), '{}-06-30'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepOli) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    MayJune = ee.ImageCollection([etmColMayJune, oliColMayJune])\n",
        "\n",
        "    etmColMayJune = MayJune.reduce('median')\n",
        "\n",
        "    etmColJulyAug = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "      .filterDate('{}-07-01'.format(year), '{}-08-31'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepEtm) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median') \\\n",
        "\n",
        "    oliColJulyAug = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
        "      .filterDate('{}-07-01'.format(year), '{}-08-31'.format(year)) \\\n",
        "      .filterBounds(points) \\\n",
        "      .map(maskQuality) \\\n",
        "      .map(prepOli) \\\n",
        "      .map(addNDVI) \\\n",
        "      .reduce('median')\n",
        "\n",
        "    JulyAug = ee.ImageCollection([etmColJulyAug, oliColJulyAug])\n",
        "\n",
        "    etmColJulyAug = JulyAug.reduce('median')\n",
        "\n",
        "    landsat7_8ImageCol = ee.ImageCollection([etmColMarchApril, etmColMayJune, etmColJulyAug])\n",
        "    return landsat7_8ImageCol\n",
        "\n",
        "\n",
        "year = 1987\n",
        "\n",
        "landsatCol = getLandsatMosaicFromPoints(year, newpts)\n",
        "\n",
        "image1 = landsatCol[0]\n",
        "image2 = landsatCol[1]\n",
        "image3 = landsatCol[2]\n",
        "\n",
        "image1_fc = image1.sampleRegions(collection=newpts, properties=['class'], scale=30)\n",
        "image2_fc = image2.sampleRegions(collection=newpts, properties=['class'], scale=30)\n",
        "image3_fc = image3.sampleRegions(collection=newpts, properties=['class'], scale=30)\n",
        "\n",
        "\n",
        "# Define a function to transfer feature properties to a dictionary.\n",
        "def fc_to_dict(fc):\n",
        "  prop_names = fc.first().propertyNames()\n",
        "  prop_lists = fc.reduceColumns(\n",
        "      reducer=ee.Reducer.toList().repeat(prop_names.size()),\n",
        "      selectors=prop_names).get('list')\n",
        "\n",
        "  return ee.Dictionary.fromLists(prop_names, prop_lists)\n",
        "\n",
        "image1_db_dict = fc_to_dict(image1_fc).getInfo()\n",
        "image2_db_dict = fc_to_dict(image2_fc).getInfo()\n",
        "image3_db_dict = fc_to_dict(image3_fc).getInfo()\n",
        "\n",
        "image1_df = pd.DataFrame(image1_db_dict)\n",
        "image2_df = pd.DataFrame(image2_db_dict)\n",
        "image3_df = pd.DataFrame(image3_db_dict)\n",
        "\n",
        "display(image1_df)\n",
        "\n",
        "data_frames = [image1_df, image2_df, image3_df]\n",
        "\n",
        "\n",
        "from functools import reduce\n",
        "df_merged = reduce(lambda left,right: pd.merge(left, right, on='system:index', how='outer'), data_frames).fillna(np.nan)\n",
        "display(df_merged)\n",
        "\n",
        "df_merged_dropna = df_merged.dropna(axis=0, how = 'any')\n",
        "#display(df_merged_dropna)\n",
        "\n",
        "\n",
        "df_merged_removeQA = df_merged_dropna.drop(['QA_PIXEL_mean_1', 'QA_RADSAT_mean_1', 'QA_PIXEL_mean_2', 'QA_RADSAT_mean_2', 'QA_PIXEL_mean_3', 'QA_RADSAT_mean_3',\n",
        "                                     'QA_PIXEL_mean_4', 'QA_RADSAT_mean_4', 'QA_PIXEL_mean_5', 'QA_RADSAT_mean_5', 'QA_PIXEL_mean_6', 'QA_RADSAT_mean_6',\n",
        "                                     'QA_PIXEL_mean_7', 'QA_RADSAT_mean_7', 'QA_PIXEL_mean_8', 'QA_RADSAT_mean_8', 'QA_PIXEL_mean_9', 'QA_RADSAT_mean_9',\n",
        "                                     'class_x', 'class_y', '.geo_x', '.geo_y', '.geo', 'system:index'], 1)\n",
        "display(df_merged_removeQA)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data_frames = [db_MarchApril2018_df, db_MayJune2018_df, db_JulyAug2018_df,\n",
        "               db_MarchApril2019_df, db_MayJune2019_df, db_JulyAug2019_df,\n",
        "               db_MarchApril2020_df, db_MayJune2020_df, db_JulyAug2020_df]\n",
        "\n",
        "from functools import reduce\n",
        "df_merged = reduce(lambda left,right: pd.merge(left, right, on='system:index', how='outer'), data_frames).fillna(np.nan)\n",
        "#display(df_merged)\n",
        "\n",
        "df_merged_dropna = df_merged.dropna(axis=0, how = 'any')\n",
        "#display(df_merged_dropna)\n",
        "\n",
        "\n",
        "df_merged_removeQA = df_merged_dropna.drop(['QA_PIXEL_mean_1', 'QA_RADSAT_mean_1', 'QA_PIXEL_mean_2', 'QA_RADSAT_mean_2', 'QA_PIXEL_mean_3', 'QA_RADSAT_mean_3',\n",
        "                                     'QA_PIXEL_mean_4', 'QA_RADSAT_mean_4', 'QA_PIXEL_mean_5', 'QA_RADSAT_mean_5', 'QA_PIXEL_mean_6', 'QA_RADSAT_mean_6',\n",
        "                                     'QA_PIXEL_mean_7', 'QA_RADSAT_mean_7', 'QA_PIXEL_mean_8', 'QA_RADSAT_mean_8', 'QA_PIXEL_mean_9', 'QA_RADSAT_mean_9',\n",
        "                                     'class_x', 'class_y', '.geo_x', '.geo_y', '.geo', 'system:index'], 1)\n",
        "display(df_merged_removeQA)\n",
        "\n",
        "\n",
        "\n",
        "# Define a function to transfer feature properties to a dictionary.\n",
        "def fc_to_dict(fc):\n",
        "  prop_names = fc.first().propertyNames()\n",
        "  prop_lists = fc.reduceColumns(\n",
        "      reducer=ee.Reducer.toList().repeat(prop_names.size()),\n",
        "      selectors=prop_names).get('list')\n",
        "\n",
        "  return ee.Dictionary.fromLists(prop_names, prop_lists)\n",
        "\n",
        "train_db_dict = fc_to_dict(subset_train_db).getInfo()\n",
        "train_df = pd.DataFrame(train_db_dict)\n",
        "display(train_df)\n",
        "#print(nbr_df.dtypes)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KQLfbROywaIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Landsat-8 Imagery from 2018, 2019, and 2020 (Centered on 2019 - corresponding to NLCD 2019 land cover classes in the training points)"
      ],
      "metadata": {
        "id": "U51ZVmh7gg6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##########\n",
        "## 2018 ##\n",
        "##########\n",
        "\n",
        "etmColMarchApril2018 = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "  .filterDate('2018-03-01', '2018-04-30') \\\n",
        "  .filterBounds(pts) \\\n",
        "  .map(maskQuality) \\\n",
        "  .map(prepEtm) \\\n",
        "  .map(addNDVI) \\\n",
        "  .reduce('mean')\n",
        "\n",
        "oliColMarchApril2018 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
        "  .filterDate('2018-03-01', '2018-04-30') \\\n",
        "  .filterBounds(pts) \\\n",
        "  .map(maskQuality) \\\n",
        "  .map(prepOli) \\\n",
        "  .map(addNDVI) \\\n",
        "  .reduce('mean')\n",
        "\n",
        "MarchApril2018 = ee.ImageCollection([etmColMarchApril2018, oliColMarchApril2018])\n",
        "\n",
        "etmColMarchApril2018 = MarchApril2018.reduce('mean')\n",
        "\n",
        "# Selects and renames bands of interest for TM/ETM+.\n",
        "def renameEtm_mean_1(img):\n",
        "  return img.select(\n",
        "      ['Blue_mean_mean', 'Green_mean_mean', 'Red_mean_mean', 'NIR_mean_mean', 'SWIR1_mean_mean', 'SWIR2_mean_mean', 'QA_PIXEL_mean_mean', 'QA_RADSAT_mean_mean', 'NDVI_mean_mean'],\n",
        "      ['Blue_mean_1', 'Green_mean_1', 'Red_mean_1', 'NIR_mean_1','SWIR1_mean_1', 'SWIR2_mean_1', 'QA_PIXEL_mean_1', 'QA_RADSAT_mean_1', 'NDVI_mean_1'])\n",
        "\n",
        "etmColMarchApril2018 = renameEtm_mean_1(etmColMarchApril2018)\n",
        "\n",
        "etmColMayJune2018 = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "  .filterDate('2018-05-01', '2018-06-30') \\\n",
        "  .filterBounds(pts) \\\n",
        "  .map(maskQuality) \\\n",
        "  .map(prepEtm) \\\n",
        "  .map(addNDVI) \\\n",
        "  .reduce('mean')\n",
        "\n",
        "oliColMayJune2018 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
        "  .filterDate('2018-05-01', '2018-06-30') \\\n",
        "  .filterBounds(pts) \\\n",
        "  .map(maskQuality) \\\n",
        "  .map(prepOli) \\\n",
        "  .map(addNDVI) \\\n",
        "  .reduce('mean')\n",
        "\n",
        "MayJune2018 = ee.ImageCollection([etmColMayJune2018, oliColMayJune2018])\n",
        "\n",
        "etmColMayJune2018 = MayJune2018.reduce('mean')\n",
        "\n",
        "# Selects and renames bands of interest for TM/ETM+.\n",
        "def renameEtm_mean_2(img):\n",
        "  return img.select(\n",
        "      ['Blue_mean_mean', 'Green_mean_mean', 'Red_mean_mean', 'NIR_mean_mean', 'SWIR1_mean_mean', 'SWIR2_mean_mean', 'QA_PIXEL_mean_mean', 'QA_RADSAT_mean_mean', 'NDVI_mean_mean'],\n",
        "      ['Blue_mean_2', 'Green_mean_2', 'Red_mean_2', 'NIR_mean_2','SWIR1_mean_2', 'SWIR2_mean_2', 'QA_PIXEL_mean_2', 'QA_RADSAT_mean_2', 'NDVI_mean_2'])\n",
        "\n",
        "etmColMayJune2018 = renameEtm_mean_2(etmColMayJune2018)\n",
        "\n",
        "etmColJulyAug2018 = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "  .filterDate('2018-07-01', '2018-08-31') \\\n",
        "  .filterBounds(pts) \\\n",
        "  .map(maskQuality) \\\n",
        "  .map(prepEtm) \\\n",
        "  .map(addNDVI) \\\n",
        "  .reduce('mean') \\\n",
        "\n",
        "oliColJulyAug2018 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
        "  .filterDate('2018-07-01', '2018-08-31') \\\n",
        "  .filterBounds(pts) \\\n",
        "  .map(maskQuality) \\\n",
        "  .map(prepOli) \\\n",
        "  .map(addNDVI) \\\n",
        "  .reduce('mean')\n",
        "\n",
        "JulyAug2018 = ee.ImageCollection([etmColJulyAug2018, oliColJulyAug2018])\n",
        "\n",
        "etmColJulyAug2018 = JulyAug2018.reduce('mean')\n",
        "\n",
        "# Selects and renames bands of interest for TM/ETM+.\n",
        "def renameEtm_mean_3(img):\n",
        "  return img.select(\n",
        "      ['Blue_mean_mean', 'Green_mean_mean', 'Red_mean_mean', 'NIR_mean_mean', 'SWIR1_mean_mean', 'SWIR2_mean_mean', 'QA_PIXEL_mean_mean', 'QA_RADSAT_mean_mean', 'NDVI_mean_mean'],\n",
        "      ['Blue_mean_3', 'Green_mean_3', 'Red_mean_3', 'NIR_mean_3','SWIR1_mean_3', 'SWIR2_mean_3', 'QA_PIXEL_mean_3', 'QA_RADSAT_mean_3', 'NDVI_mean_3'])\n",
        "\n",
        "etmColJulyAug2018 = renameEtm_mean_3(etmColJulyAug2018)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########\n",
        "## 2019 ##\n",
        "##########\n",
        "\n",
        "etmColMarchApril2019 = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "  .filterDate('2019-03-01', '2019-04-30') \\\n",
        "  .filterBounds(pts) \\\n",
        "  .map(maskQuality) \\\n",
        "  .map(prepEtm) \\\n",
        "  .map(addNDVI) \\\n",
        "  .reduce('mean')\n",
        "\n",
        "oliColMarchApril2019 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
        "  .filterDate('2019-03-01', '2019-04-30') \\\n",
        "  .filterBounds(pts) \\\n",
        "  .map(maskQuality) \\\n",
        "  .map(prepOli) \\\n",
        "  .map(addNDVI) \\\n",
        "  .reduce('mean')\n",
        "\n",
        "MarchApril2019 = ee.ImageCollection([etmColMarchApril2019, oliColMarchApril2019])\n",
        "\n",
        "etmColMarchApril2019 = MarchApril2019.reduce('mean')\n",
        "\n",
        "# Selects and renames bands of interest for TM/ETM+.\n",
        "def renameEtm_mean_4(img):\n",
        "  return img.select(\n",
        "      ['Blue_mean_mean', 'Green_mean_mean', 'Red_mean_mean', 'NIR_mean_mean', 'SWIR1_mean_mean', 'SWIR2_mean_mean', 'QA_PIXEL_mean_mean', 'QA_RADSAT_mean_mean', 'NDVI_mean_mean'],\n",
        "      ['Blue_mean_4', 'Green_mean_4', 'Red_mean_4', 'NIR_mean_4','SWIR1_mean_4', 'SWIR2_mean_4', 'QA_PIXEL_mean_4', 'QA_RADSAT_mean_4', 'NDVI_mean_4'])\n",
        "\n",
        "etmColMarchApril2019 = renameEtm_mean_4(etmColMarchApril2019)\n",
        "\n",
        "etmColMayJune2019 = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "  .filterDate('2019-05-01', '2019-06-30') \\\n",
        "  .filterBounds(pts) \\\n",
        "  .map(maskQuality) \\\n",
        "  .map(prepEtm) \\\n",
        "  .map(addNDVI) \\\n",
        "  .reduce('mean')\n",
        "\n",
        "oliColMayJune2019 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
        "  .filterDate('2019-05-01', '2019-06-30') \\\n",
        "  .filterBounds(pts) \\\n",
        "  .map(maskQuality) \\\n",
        "  .map(prepOli) \\\n",
        "  .map(addNDVI) \\\n",
        "  .reduce('mean')\n",
        "\n",
        "MayJune2019 = ee.ImageCollection([etmColMayJune2019, oliColMayJune2019])\n",
        "\n",
        "etmColMayJune2019 = MayJune2019.reduce('mean')\n",
        "\n",
        "# Selects and renames bands of interest for TM/ETM+.\n",
        "def renameEtm_mean_5(img):\n",
        "  return img.select(\n",
        "      ['Blue_mean_mean', 'Green_mean_mean', 'Red_mean_mean', 'NIR_mean_mean', 'SWIR1_mean_mean', 'SWIR2_mean_mean', 'QA_PIXEL_mean_mean', 'QA_RADSAT_mean_mean', 'NDVI_mean_mean'],\n",
        "      ['Blue_mean_5', 'Green_mean_5', 'Red_mean_5', 'NIR_mean_5','SWIR1_mean_5', 'SWIR2_mean_5', 'QA_PIXEL_mean_5', 'QA_RADSAT_mean_5', 'NDVI_mean_5'])\n",
        "\n",
        "etmColMayJune2019 = renameEtm_mean_5(etmColMayJune2019)\n",
        "\n",
        "etmColJulyAug2019 = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "  .filterDate('2019-07-01', '2019-08-31') \\\n",
        "  .filterBounds(pts) \\\n",
        "  .map(maskQuality) \\\n",
        "  .map(prepEtm) \\\n",
        "  .map(addNDVI) \\\n",
        "  .reduce('mean') \\\n",
        "\n",
        "oliColJulyAug2019 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
        "  .filterDate('2019-07-01', '2019-08-31') \\\n",
        "  .filterBounds(pts) \\\n",
        "  .map(maskQuality) \\\n",
        "  .map(prepOli) \\\n",
        "  .map(addNDVI) \\\n",
        "  .reduce('mean')\n",
        "\n",
        "JulyAug2019 = ee.ImageCollection([etmColJulyAug2019, oliColJulyAug2019])\n",
        "\n",
        "etmColJulyAug2019 = JulyAug2019.reduce('mean')\n",
        "\n",
        "# Selects and renames bands of interest for TM/ETM+.\n",
        "def renameEtm_mean_6(img):\n",
        "  return img.select(\n",
        "      ['Blue_mean_mean', 'Green_mean_mean', 'Red_mean_mean', 'NIR_mean_mean', 'SWIR1_mean_mean', 'SWIR2_mean_mean', 'QA_PIXEL_mean_mean', 'QA_RADSAT_mean_mean', 'NDVI_mean_mean'],\n",
        "      ['Blue_mean_6', 'Green_mean_6', 'Red_mean_6', 'NIR_mean_6','SWIR1_mean_6', 'SWIR2_mean_6', 'QA_PIXEL_mean_6', 'QA_RADSAT_mean_6', 'NDVI_mean_6'])\n",
        "\n",
        "etmColJulyAug2019 = renameEtm_mean_6(etmColJulyAug2019)\n",
        "\n",
        "\n",
        "##########\n",
        "## 2020 ##\n",
        "##########\n",
        "\n",
        "etmColMarchApril2020 = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "  .filterDate('2020-03-01', '2020-04-30') \\\n",
        "  .filterBounds(pts) \\\n",
        "  .map(maskQuality) \\\n",
        "  .map(prepEtm) \\\n",
        "  .map(addNDVI) \\\n",
        "  .reduce('mean')\n",
        "\n",
        "oliColMarchApril2020 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
        "  .filterDate('2020-03-01', '2020-04-30') \\\n",
        "  .filterBounds(pts) \\\n",
        "  .map(maskQuality) \\\n",
        "  .map(prepOli) \\\n",
        "  .map(addNDVI) \\\n",
        "  .reduce('mean')\n",
        "\n",
        "MarchApril2020 = ee.ImageCollection([etmColMarchApril2020, oliColMarchApril2020])\n",
        "\n",
        "etmColMarchApril2020 = MarchApril2020.reduce('mean')\n",
        "\n",
        "# Selects and renames bands of interest for TM/ETM+.\n",
        "def renameEtm_mean_7(img):\n",
        "  return img.select(\n",
        "      ['Blue_mean_mean', 'Green_mean_mean', 'Red_mean_mean', 'NIR_mean_mean', 'SWIR1_mean_mean', 'SWIR2_mean_mean', 'QA_PIXEL_mean_mean', 'QA_RADSAT_mean_mean', 'NDVI_mean_mean'],\n",
        "      ['Blue_mean_7', 'Green_mean_7', 'Red_mean_7', 'NIR_mean_7','SWIR1_mean_7', 'SWIR2_mean_7', 'QA_PIXEL_mean_7', 'QA_RADSAT_mean_7', 'NDVI_mean_7'])\n",
        "\n",
        "etmColMarchApril2020 = renameEtm_mean_7(etmColMarchApril2020)\n",
        "\n",
        "etmColMayJune2020 = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "  .filterDate('2020-05-01', '2020-06-30') \\\n",
        "  .filterBounds(pts) \\\n",
        "  .map(maskQuality) \\\n",
        "  .map(prepEtm) \\\n",
        "  .map(addNDVI) \\\n",
        "  .reduce('mean')\n",
        "\n",
        "oliColMayJune2020 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
        "  .filterDate('2020-05-01', '2020-06-30') \\\n",
        "  .filterBounds(pts) \\\n",
        "  .map(maskQuality) \\\n",
        "  .map(prepOli) \\\n",
        "  .map(addNDVI) \\\n",
        "  .reduce('mean')\n",
        "\n",
        "MayJune2020 = ee.ImageCollection([etmColMayJune2020, oliColMayJune2020])\n",
        "\n",
        "etmColMayJune2020 = MayJune2020.reduce('mean')\n",
        "\n",
        "# Selects and renames bands of interest for TM/ETM+.\n",
        "def renameEtm_mean_8(img):\n",
        "  return img.select(\n",
        "      ['Blue_mean_mean', 'Green_mean_mean', 'Red_mean_mean', 'NIR_mean_mean', 'SWIR1_mean_mean', 'SWIR2_mean_mean', 'QA_PIXEL_mean_mean', 'QA_RADSAT_mean_mean', 'NDVI_mean_mean'],\n",
        "      ['Blue_mean_8', 'Green_mean_8', 'Red_mean_8', 'NIR_mean_8','SWIR1_mean_8', 'SWIR2_mean_8', 'QA_PIXEL_mean_8', 'QA_RADSAT_mean_8', 'NDVI_mean_8'])\n",
        "\n",
        "etmColMayJune2020 = renameEtm_mean_8(etmColMayJune2020)\n",
        "\n",
        "etmColJulyAug2020 = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
        "  .filterDate('2020-07-01', '2020-08-31') \\\n",
        "  .filterBounds(pts) \\\n",
        "  .map(maskQuality) \\\n",
        "  .map(prepEtm) \\\n",
        "  .map(addNDVI) \\\n",
        "  .reduce('mean') \\\n",
        "\n",
        "oliColJulyAug2020 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
        "  .filterDate('2020-07-01', '2020-08-31') \\\n",
        "  .filterBounds(pts) \\\n",
        "  .map(maskQuality) \\\n",
        "  .map(prepOli) \\\n",
        "  .map(addNDVI) \\\n",
        "  .reduce('mean')\n",
        "\n",
        "JulyAug2020 = ee.ImageCollection([etmColJulyAug2020, oliColJulyAug2020])\n",
        "\n",
        "etmColJulyAug2020 = JulyAug2020.reduce('mean')\n",
        "\n",
        "# Selects and renames bands of interest for TM/ETM+.\n",
        "def renameEtm_mean_9(img):\n",
        "  return img.select(\n",
        "      ['Blue_mean_mean', 'Green_mean_mean', 'Red_mean_mean', 'NIR_mean_mean', 'SWIR1_mean_mean', 'SWIR2_mean_mean', 'QA_PIXEL_mean_mean', 'QA_RADSAT_mean_mean', 'NDVI_mean_mean'],\n",
        "      ['Blue_mean_9', 'Green_mean_9', 'Red_mean_9', 'NIR_mean_9','SWIR1_mean_9', 'SWIR2_mean_9', 'QA_PIXEL_mean_9', 'QA_RADSAT_mean_9', 'NDVI_mean_9'])\n",
        "\n",
        "etmColJulyAug2020 = renameEtm_mean_9(etmColJulyAug2020)\n",
        "\n",
        "etmColJulyAug2020.getInfo()\n"
      ],
      "metadata": {
        "id": "9oqLZm-Helq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extract Landsat-8 Bands from Points, Format into Pandas DataFrame"
      ],
      "metadata": {
        "id": "U_HkdxmEhkeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Sample Regions\n",
        "\n",
        "# 2018\n",
        "etmColMarchApril2018_fc = etmColMarchApril2018.sampleRegions(collection=pts, properties=['class'], scale=30)\n",
        "#print(etmColMarchApril2018_fc.getInfo())\n",
        "etmColMayJune2018_fc = etmColMayJune2018.sampleRegions(collection=pts, properties=['class'], scale=30, geometries=True)\n",
        "#print(etmColMayJune2018_fc.getInfo())\n",
        "etmColJulyAug2018_fc = etmColJulyAug2018.sampleRegions(collection=pts, properties=['class'], scale=30, geometries=True)\n",
        "#print(etmColJulyAug2018_fc.getInfo())\n",
        "\n",
        "# 2019\n",
        "etmColMarchApril2019_fc = etmColMarchApril2019.sampleRegions(collection=pts, properties=['class'], scale=30, geometries=True)\n",
        "#print(etmColMarchApril2019_fc.getInfo())\n",
        "etmColMayJune2019_fc = etmColMayJune2019.sampleRegions(collection=pts, properties=['class'], scale=30, geometries=True)\n",
        "#print(etmColMayJune2019_fc.getInfo())\n",
        "etmColJulyAug2019_fc = etmColJulyAug2019.sampleRegions(collection=pts, properties=['class'], scale=30, geometries=True)\n",
        "#print(etmColJulyAug2019_fc.getInfo())\n",
        "\n",
        "# 2020\n",
        "etmColMarchApril2020_fc = etmColMarchApril2020.sampleRegions(collection=pts, properties=['class'], scale=30, geometries=True)\n",
        "#print(etmColMarchApril2020_fc.getInfo())\n",
        "etmColMayJune2020_fc = etmColMayJune2020.sampleRegions(collection=pts, properties=['class'], scale=30, geometries=True)\n",
        "#print(etmColMayJune2020_fc.getInfo())\n",
        "etmColJulyAug2020_fc = etmColJulyAug2020.sampleRegions(collection=pts, properties=['class'], scale=30, geometries=True)\n",
        "#print(etmColJulyAug2020_fc.getInfo())\n",
        "\n",
        "\n",
        "\n",
        "# Export feature class assets\n",
        "outputBucket = 'landcover_samples_nlcd2019_onemillionpoints'\n",
        "# Make sure the bucket exists.\n",
        "print('Found Cloud Storage bucket.' if tf.io.gfile.exists('gs://' + outputBucket) \n",
        "    else 'Output Cloud Storage bucket does not exist.')\n",
        "\n",
        "\n",
        "# 2018\n",
        "task1 = ee.batch.Export.table.toCloudStorage(\n",
        "    collection=etmColMarchApril2018_fc,\n",
        "    description='etmColMarchApril2018_fc_landcover export',\n",
        "    bucket = outputBucket,\n",
        "    fileNamePrefix='etmColMarchApril2018_fc_landcover',\n",
        "    fileFormat='CSV')\n",
        "\n",
        "task2 = ee.batch.Export.table.toCloudStorage(\n",
        "    collection=etmColMayJune2018_fc,\n",
        "    description='etmColMayJune2018_fc_landcover export',\n",
        "    bucket = outputBucket,\n",
        "    fileNamePrefix='etmColMayJune2018_fc_landcover',\n",
        "    fileFormat='CSV')\n",
        "\n",
        "task3 = ee.batch.Export.table.toCloudStorage(\n",
        "    collection=etmColJulyAug2018_fc,\n",
        "    description='etmColJulyAug2018_fc_landcover export',\n",
        "    bucket = outputBucket,\n",
        "    fileNamePrefix='etmColJulyAug2018_fc_landcover',\n",
        "    fileFormat='CSV')\n",
        "\n",
        "# 2019\n",
        "task4 = ee.batch.Export.table.toCloudStorage(\n",
        "    collection=etmColMarchApril2019_fc,\n",
        "    description='etmColMarchApril2019_fc_landcover export',\n",
        "    bucket = outputBucket,\n",
        "    fileNamePrefix='etmColMarchApril2019_fc_landcover',\n",
        "    fileFormat='CSV')\n",
        "\n",
        "task5 = ee.batch.Export.table.toCloudStorage(\n",
        "    collection=etmColMayJune2019_fc,\n",
        "    description='etmColMayJune2019_fc_landcover export',\n",
        "    bucket = outputBucket,\n",
        "    fileNamePrefix='etmColMayJune2019_fc_landcover',\n",
        "    fileFormat='CSV')\n",
        "\n",
        "task6 = ee.batch.Export.table.toCloudStorage(\n",
        "    collection=etmColJulyAug2019_fc,\n",
        "    description='etmColJulyAug2019_fc_landcover export',\n",
        "    bucket = outputBucket,\n",
        "    fileNamePrefix='etmColJulyAug2019_fc_landcover',\n",
        "    fileFormat='CSV')\n",
        "\n",
        "# 2020\n",
        "task7 = ee.batch.Export.table.toCloudStorage(\n",
        "    collection=etmColMarchApril2020_fc,\n",
        "    description='etmColMarchApril2020_fc_landcover export',\n",
        "    bucket = outputBucket,\n",
        "    fileNamePrefix='etmColMarchApril2020_fc_landcover',\n",
        "    fileFormat='CSV')\n",
        "\n",
        "task8 = ee.batch.Export.table.toCloudStorage(\n",
        "    collection=etmColMayJune2020_fc,\n",
        "    description='etmColMayJune2020_fc_landcover export',\n",
        "    bucket = outputBucket,\n",
        "    fileNamePrefix='etmColMayJune2020_fc_landcover',\n",
        "    fileFormat='CSV')\n",
        "\n",
        "task9 = ee.batch.Export.table.toCloudStorage(\n",
        "    collection=etmColJulyAug2020_fc,\n",
        "    description='etmColJulyAug2020_fc_landcover export',\n",
        "    bucket = outputBucket,\n",
        "    fileNamePrefix='etmColJulyAug2020_fc_landcover',\n",
        "    fileFormat='CSV')\n",
        "\n",
        "#Export/Start Tasks\n",
        "\n",
        "task1.start()\n",
        "task2.start()\n",
        "task3.start()\n",
        "task4.start()\n",
        "task5.start()\n",
        "task6.start()\n",
        "task7.start()\n",
        "task8.start()\n",
        "task9.start()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0d4WryBWelsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Re-Import Feature Class Assets (no longer hosted on EE and thus not restricted to size limits) and format to pandas DataFrame\n",
        "# Depends on import gcsfs\n",
        "\n",
        "#2018\n",
        "db_MarchApril2018_df = pd.read_csv('gs://landcover_samples_nlcd2019_onemillionpoints/etmColMarchApril2018_fc_landcover.csv')\n",
        "#display(db_MarchApril2018_df)\n",
        "db_MayJune2018_df = pd.read_csv('gs://landcover_samples_nlcd2019_onemillionpoints/etmColMayJune2018_fc_landcover.csv')\n",
        "#display(db_MayJune2018_df)\n",
        "db_JulyAug2018_df = pd.read_csv('gs://landcover_samples_nlcd2019_onemillionpoints/etmColJulyAug2018_fc_landcover.csv')\n",
        "#display(db_JulyAug2018_df)\n",
        "\n",
        "\n",
        "#2019\n",
        "db_MarchApril2019_df = pd.read_csv('gs://landcover_samples_nlcd2019_onemillionpoints/etmColMarchApril2019_fc_landcover.csv')\n",
        "#display(db_MarchApril2019_df)\n",
        "db_MayJune2019_df = pd.read_csv('gs://landcover_samples_nlcd2019_onemillionpoints/etmColMayJune2019_fc_landcover.csv')\n",
        "#display(db_MayJune2019_df)\n",
        "db_JulyAug2019_df = pd.read_csv('gs://landcover_samples_nlcd2019_onemillionpoints/etmColJulyAug2019_fc_landcover.csv')\n",
        "#display(db_JulyAug2019_df)\n",
        "\n",
        "\n",
        "#2020\n",
        "db_MarchApril2020_df = pd.read_csv('gs://landcover_samples_nlcd2019_onemillionpoints/etmColMarchApril2020_fc_landcover.csv')\n",
        "#display(db_MarchApril2020_df)\n",
        "db_MayJune2020_df = pd.read_csv('gs://landcover_samples_nlcd2019_onemillionpoints/etmColMayJune2020_fc_landcover.csv')\n",
        "#display(db_MayJune2020_df)\n",
        "db_JulyAug2020_df = pd.read_csv('gs://landcover_samples_nlcd2019_onemillionpoints/etmColJulyAug2020_fc_landcover.csv')\n",
        "#display(db_JulyAug2020_df)"
      ],
      "metadata": {
        "id": "Fj5ZW3WY3O7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Merge Landsat-8 <> LandCover Points into one DataFrame"
      ],
      "metadata": {
        "id": "_o8Jy7tliW8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_frames = [db_MarchApril2018_df, db_MayJune2018_df, db_JulyAug2018_df,\n",
        "               db_MarchApril2019_df, db_MayJune2019_df, db_JulyAug2019_df,\n",
        "               db_MarchApril2020_df, db_MayJune2020_df, db_JulyAug2020_df]\n",
        "\n",
        "from functools import reduce\n",
        "df_merged = reduce(lambda left,right: pd.merge(left, right, on='system:index', how='outer'), data_frames).fillna(np.nan)\n",
        "#display(df_merged)\n",
        "\n",
        "df_merged_dropna = df_merged.dropna(axis=0, how = 'any')\n",
        "#display(df_merged_dropna)\n",
        "\n",
        "\n",
        "df_merged_removeQA = df_merged_dropna.drop(['QA_PIXEL_mean_1', 'QA_RADSAT_mean_1', 'QA_PIXEL_mean_2', 'QA_RADSAT_mean_2', 'QA_PIXEL_mean_3', 'QA_RADSAT_mean_3',\n",
        "                                     'QA_PIXEL_mean_4', 'QA_RADSAT_mean_4', 'QA_PIXEL_mean_5', 'QA_RADSAT_mean_5', 'QA_PIXEL_mean_6', 'QA_RADSAT_mean_6',\n",
        "                                     'QA_PIXEL_mean_7', 'QA_RADSAT_mean_7', 'QA_PIXEL_mean_8', 'QA_RADSAT_mean_8', 'QA_PIXEL_mean_9', 'QA_RADSAT_mean_9',\n",
        "                                     'class_x', 'class_y', '.geo_x', '.geo_y', '.geo', 'system:index'], 1)\n",
        "display(df_merged_removeQA)\n",
        "\n"
      ],
      "metadata": {
        "id": "ARb8v3H2elvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Export DataFrame to CSV"
      ],
      "metadata": {
        "id": "3db0Ks7NjLtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged_removeQA.to_csv(\"example2_1dcnn_april2022.csv\")"
      ],
      "metadata": {
        "id": "HVYg2R-YelxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions to read and compute spectral features on SITS \n"
      ],
      "metadata": {
        "id": "7g9aRjCnjzUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\" \n",
        "\tSome functions to read and compute spectral features on SITS\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import sys, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "import csv\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "#---------------------- SATELLITE MODULE\n",
        "#-----------------------------------------------------------------------\n",
        "#final_class_label = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9', 'c10', 'c11', 'c12']\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "def readSITSData(name_file):\n",
        "\t\"\"\"\n",
        "\t\tRead the data contained in name_file\n",
        "\t\tINPUT:\n",
        "\t\t\t- name_file: file where to read the data\n",
        "\t\tOUTPUT:\n",
        "\t\t\t- X: variable vectors for each example\n",
        "\t\t\t- polygon_ids: id polygon (use e.g. for validation set)\n",
        "\t\t\t- Y: label for each example\n",
        "\t\"\"\"\n",
        "\t\n",
        "\tdata = pd.read_table(name_file, sep=',', header=None)\n",
        "\t\n",
        "\ty_data = data.iloc[:,0]\n",
        "\ty = np.asarray(y_data.values, dtype='uint8')\n",
        "\t\n",
        "\tpolygonID_data = data.iloc[:,1]\n",
        "\tpolygon_ids = polygonID_data.values\n",
        "\tpolygon_ids = np.asarray(polygon_ids, dtype='uint16')\n",
        "\t\t\n",
        "\tX_data = data.iloc[:,2:]\n",
        "\tX = X_data.values\n",
        "\tX = np.asarray(X, dtype='float32')\n",
        "\n",
        "\treturn  X, polygon_ids, y\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "def addFeatures(X):\n",
        "\t\"\"\"\n",
        "\t\tRead the data contained in name_file\n",
        "\t\tINPUT:\n",
        "\t\t\t- X: orginal X features composed of threes bands (NIR-R-G)\n",
        "\t\t\t\tin the following order \n",
        "\t\t\t\t\t[date1.NIR, date1.R, date1.G, ..., dateD.NIR, dateD.R, dateD.G]\n",
        "\t\tOUTPUT:\n",
        "\t\t\t- X_features: orginal_X with the addition of NDVI, NDWI and Brilliance\n",
        "\t\t\t\tin the following order\t\n",
        "\t\t\t\t\t[X, date1.NDVI, ..., dateD.NDVI, date1.NDWI, ..., dateD.NDWI, date1.Brilliance, ..., dateD.Brilliance]\n",
        "\t\"\"\"\n",
        "\tn_channels = 3\n",
        "\t\n",
        "\tNIR = X[:,0::n_channels]\n",
        "\tNIR = np.array(NIR)\n",
        "\tNIR = NIR.astype(np.float)\n",
        "\tR = X[:,1::n_channels]\n",
        "\tR = np.array(R)\n",
        "\tR = R.astype(np.float)\n",
        "\tG = X[:,2::n_channels]\n",
        "\tG = np.array(G)\n",
        "\tG = G.astype(np.float)\t\n",
        "\t\n",
        "\tNDVI = np.where(NIR+R!=0., (NIR-R)/(NIR+R), 0.)\n",
        "\tNDVI = NDVI.astype(float)\n",
        "\t\n",
        "\t\n",
        "\tNDWI = np.where(G+NIR!=0., (G-NIR)/(G+NIR), 0.)\n",
        "\tNDWI = NDWI.astype(float)\n",
        "\t\n",
        "\tBrilliance = np.sqrt((NIR*NIR + R*R + G*G)/3.0)\n",
        "\tBrilliance = Brilliance.astype(float)\n",
        "\t\n",
        "\treturn NDVI, NDWI, Brilliance\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "def computeNDVI(X, n_channels):\n",
        "\t\"\"\"\n",
        "\t\tRead the data contained in name_file\n",
        "\t\tINPUT:\n",
        "\t\t\t- X: orginal X features composed of threes bands (NIR-R-G)\n",
        "\t\t\t\tin the following order \n",
        "\t\t\t\t\t[date1.NIR, date1.R, date1.G, ..., dateD.NIR, dateD.R, dateD.G]\n",
        "\t\tOUTPUT:\n",
        "\t\t\t- X_features: orginal_X with the addition of NDVI, NDWI and Brilliance\n",
        "\t\t\t\tin the following order\t\n",
        "\t\t\t\t\t[X, date1.NDVI, ..., dateD.NDVI, date1.NDWI, ..., dateD.NDWI, date1.Brilliance, ..., dateD.Brilliance]\n",
        "\t\"\"\"\n",
        "\t\n",
        "\tNIR = X[:,0::n_channels]\n",
        "\tNIR = np.array(NIR)\n",
        "\tNIR = NIR.astype(np.float)\n",
        "\tR = X[:,1::n_channels]\n",
        "\tR = np.array(R)\n",
        "\tR = R.astype(np.float)\n",
        "\t\n",
        "\tNDVI = np.where(NIR+R!=0., (NIR-R)/(NIR+R), 0.)\n",
        "\treturn NDVI\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "def addingfeat_reshape_data(X, feature_strategy, nchannels):\n",
        "\t\"\"\"\n",
        "\t\tReshaping (feature format (3 bands): d1.b1 d1.b2 d1.b3 d2.b1 d2.b2 d2.b3 ...)\n",
        "\t\tINPUT:\n",
        "\t\t\t-X: original feature vector ()\n",
        "\t\t\t-feature_strategy: used features (options: SB, NDVI, SB3feat)\n",
        "\t\t\t-nchannels: number of channels\n",
        "\t\tOUTPUT:\n",
        "\t\t\t-new_X: data in the good format for Keras models\n",
        "\t\"\"\"\n",
        "\t\t\t\n",
        "\tif feature_strategy=='SB':\n",
        "\t\tprint(\"SPECTRAL BANDS-----------------------------------------\")\n",
        "\t\treturn X.reshape(X.shape[0],int(X.shape[1]/nchannels),nchannels)\n",
        "\t\t\t\t\t\t\t\t\n",
        "\telif feature_strategy=='NDVI':\n",
        "\t\tprint(\"NDVI only----------------------------------------------\")\n",
        "\t\tnew_X = computeNDVI(X, nchannels)\n",
        "\t\treturn np.expand_dims(new_X, axis=2)\n",
        "\t\t\t\t\t\t\t\n",
        "\telif feature_strategy=='SB3feat':\n",
        "\t\tprint(\"SB + NDVI + NDWI + IB----------------------------------\")\n",
        "\t\tNDVI, NDWI, IB = addFeatures(X)\t\t\n",
        "\t\tnew_X = X.reshape(X.shape[0],int(X.shape[1]/nchannels),nchannels)\t\t\n",
        "\t\tnew_X = np.dstack((new_X, NDVI))\n",
        "\t\tnew_X = np.dstack((new_X, NDWI))\n",
        "\t\tnew_X = np.dstack((new_X, IB))\n",
        "\t\treturn new_X\n",
        "\telse:\n",
        "\t\tprint(\"Not referenced!!!-------------------------------------------\")\n",
        "\t\treturn -1\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "def computingMinMax(X, per=2):\n",
        "\tmin_per = np.percentile(X, per, axis=(0,1))\n",
        "\tmax_per = np.percentile(X, 100-per, axis=(0,1))\n",
        "\treturn min_per, max_per\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "def read_minMaxVal(minmax_file):\t\n",
        "\twith open(minmax_file, 'r') as f:\n",
        "\t\treader = csv.reader(f, delimiter=',')\n",
        "\t\tmin_per = next(reader)\n",
        "\t\tmax_per = next(reader)\n",
        "\tmin_per = [float(k) for k in min_per]\n",
        "\tmin_per = np.array(min_per)\n",
        "\tmax_per = [float(k) for k in max_per]\n",
        "\tmax_per = np.array(max_per)\n",
        "\treturn min_per, max_per\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "def save_minMaxVal(minmax_file, min_per, max_per):\t\n",
        "\twith open(minmax_file, 'w') as f:\n",
        "\t\twriter = csv.writer(f, delimiter=',')\n",
        "\t\twriter.writerow(min_per)\n",
        "\t\twriter.writerow(max_per)\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "def normalizingData(X, min_per, max_per):\n",
        "\treturn (X-min_per)/(max_per-min_per)\n",
        "\n",
        "#-----------------------------------------------------------------------\t\n",
        "def extractValSet(X_train, polygon_ids_train, y_train, val_rate=0.1):\n",
        "\tunique_pol_ids_train, indices = np.unique(polygon_ids_train, return_inverse=True) #-- pold_ids_train = unique_pol_ids_train[indices]\n",
        "\tnb_pols = len(unique_pol_ids_train)\n",
        "\t\n",
        "\tind_shuffle = list(range(nb_pols))\n",
        "\trandom.shuffle(ind_shuffle)\n",
        "\tlist_indices = [[] for i in range(nb_pols)]\n",
        "\tshuffle_indices = [[] for i in range(nb_pols)]\n",
        "\t[ list_indices[ind_shuffle[val]].append(idx) for idx, val in enumerate(indices)]\t\t\t\t\t\n",
        "\t\t\n",
        "\tfinal_ind = list(itertools.chain.from_iterable(list_indices))\n",
        "\tm = len(final_ind)\n",
        "\tfinal_train = int(math.ceil(m*(1.0-val_rate)))\n",
        "\t\n",
        "\tshuffle_polygon_ids_train = polygon_ids_train[final_ind]\n",
        "\tid_final_train = shuffle_polygon_ids_train[final_train]\n",
        "\t\n",
        "\twhile shuffle_polygon_ids_train[final_train-1]==id_final_train:\n",
        "\t\tfinal_train = final_train-1\n",
        "\t\n",
        "\t\n",
        "\tnew_X_train = X_train[final_ind[:final_train],:,:]\n",
        "\tnew_y_train = y_train[final_ind[:final_train]]\n",
        "\tnew_X_val = X_train[final_ind[final_train:],:,:]\n",
        "\tnew_y_val = y_train[final_ind[final_train:]]\n",
        "\t\n",
        "\treturn new_X_train, new_y_train, new_X_val, new_y_val\n",
        "\t"
      ],
      "metadata": {
        "id": "PMlD11bnelzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "open(\"/content/drive/My Drive/Invasives Research UMN/Remote Sensing Master/Leafy Spurge Demography/temporalCNN-master/example/train_dataset.csv\").read()\n",
        "\n",
        "res_path = '/content/drive/My Drive/Invasives Research UMN/Remote Sensing Master/Leafy Spurge Demography'\n",
        "sits_path = '/content/drive/My Drive/Invasives Research UMN/Remote Sensing Master/Leafy Spurge Demography/temporalCNN-master/example'\n",
        "feature = \"SB\"\n",
        "noarchi = 2\n",
        "norun = 0\n",
        "\n",
        "\n",
        "#-- Creating output path if does not exist\n",
        "if not os.path.exists(res_path):\n",
        "  print(\"ResPath DNE\")\n",
        "  os.makedirs(res_path)\n",
        "\t\n",
        "\t#---- Parameters to set\n",
        "n_channels = 7 #-- B G NDVI NIR Red SWIR1 SWIR2\n",
        "val_rate = 0.00\n",
        "\n",
        "\t#---- Evaluated metrics\n",
        "eval_label = ['OA', 'train_loss', 'train_time', 'test_time']\t\n",
        "\t\n",
        "\t#---- String variables\n",
        "train_str = 'train_dataset'\n",
        "test_str = 'test_dataset'\t\t\t\t\t\n",
        "\t#---- Get filenames\n",
        "train_file = sits_path + '/' + train_str + '.csv'\n",
        "test_file = sits_path + '/' + test_str + '.csv'\n",
        "print(\"train_file: \", train_file)\n",
        "print(\"test_file: \", test_file)\n",
        "\t\n",
        "\t#---- output files\t\t\t\n",
        "res_path = res_path + '/Archi' + str(noarchi) + '/'\n",
        "if not os.path.exists(res_path):\n",
        "  os.makedirs(res_path)\n",
        "  print(\"noarchi: \", noarchi)\n",
        "\t\n",
        "str_result = feature + '-' + train_str + '-noarchi' + str(noarchi) + '-norun' + str(norun) \n",
        "res_file = res_path + '/resultOA-' + str_result + '.csv'\n",
        "res_mat = np.zeros((len(eval_label),1))\n",
        "traintest_loss_file = res_path + '/trainingHistory-' + str_result + '.csv'\n",
        "conf_file = res_path + '/confMatrix-' + str_result + '.csv'\n",
        "out_model_file = res_path + '/bestmodel-' + str_result + '.h5'\n",
        "\n",
        "\n",
        "\t#---- Downloading\n",
        "X_train, polygon_ids_train, y_train = readSITSData(train_file)\n",
        "\n",
        "print(X_train.shape) #13336, 63\n",
        "X_test,  polygon_ids_test, y_test = readSITSData(test_file)\n",
        "print(X_train)\n",
        "print(polygon_ids_train)\n",
        "print(y_train.shape) #13336\n",
        "\n",
        "n_classes_test = len(np.unique(y_test))\n",
        "print(n_classes_test)\n",
        "n_classes_train = len(np.unique(y_train))\n",
        "print(n_classes_train)\n",
        "if(n_classes_test != n_classes_train):\n",
        "  print(\"WARNING: different number of classes in train and test\")\n",
        "n_classes = max(n_classes_train, n_classes_test)\n",
        "y_train_one_hot = to_categorical(y_train) #specify number of classes explicity - may need to recode classes sequentially (1-9) to work correctly\n",
        "y_test_one_hot = to_categorical(y_test)\n",
        "\n",
        "print(y_train_one_hot)\n",
        "print(y_test_one_hot)\n",
        "\t\n",
        "\t#---- Adding the features and reshaping the data if necessary\n",
        "X_train = addingfeat_reshape_data(X_train, feature, n_channels) #Feature = \"SB\" (spectral bands)\n",
        "\n",
        "print(X_train[0, :, :])\n",
        "print(X_train.shape)\n",
        "X_test = addingfeat_reshape_data(X_test, feature, n_channels)\t\t\n",
        "print(X_test.shape)\n",
        "\n",
        "#---- Normalizing the data per band (Do we want to normalize across years or within one year?)\n",
        "minMaxVal_file = '.'.join(out_model_file.split('.')[0:-1])\n",
        "minMaxVal_file = minMaxVal_file + '_minMax.txt'\n",
        "\n",
        "if not os.path.exists(minMaxVal_file): \n",
        "  min_per, max_per = computingMinMax(X_train) #compute 98% min/max (per = 2) on bands\n",
        "  save_minMaxVal(minMaxVal_file, min_per, max_per)\n",
        "else:\n",
        "  min_per, max_per = read_minMaxVal(minMaxVal_file)\n",
        "\n",
        "print(min_per, max_per)\n",
        "\n",
        "X_train =  normalizingData(X_train, min_per, max_per)\n",
        "X_test =  normalizingData(X_test, min_per, max_per)\n",
        "\n",
        "print(X_train) #verify normalization worked as intended\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OU_UXMDzkVni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Keras Model Architectures\n",
        "\n",
        "https://github.com/charlotte-pel/temporalCNN/\n"
      ],
      "metadata": {
        "id": "JVWSC7qXuSW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\" \n",
        "\tDefining keras architecre, and training the models\n",
        "\"\"\"\n",
        "\n",
        "import sys, os\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Input, Dense, Activation, BatchNormalization, Dropout, Flatten, Lambda, SpatialDropout1D, Concatenate\n",
        "from keras.layers import Conv1D, Conv2D, AveragePooling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from keras.callbacks import Callback, ModelCheckpoint, History, EarlyStopping\n",
        "from keras.models import Model, load_model\n",
        "from keras.optimizers import *\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "#---------------------- Modules\n",
        "#-----------------------------------------------------------------------\n",
        "\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def conv_bn(X, **conv_params):\t\n",
        "\tnbunits = conv_params[\"nbunits\"];\n",
        "\tkernel_size = conv_params[\"kernel_size\"];\n",
        "\n",
        "\tstrides = conv_params.setdefault(\"strides\", 1)\n",
        "\tpadding = conv_params.setdefault(\"padding\", \"same\")\n",
        "\tkernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-6))\n",
        "\tkernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "\n",
        "\tZ = Conv1D(nbunits, kernel_size=kernel_size, \n",
        "\t\t\tstrides = strides, padding=padding,\n",
        "\t\t\tkernel_initializer=kernel_initializer,\n",
        "\t\t\tkernel_regularizer=kernel_regularizer)(X)\n",
        "\n",
        "\treturn BatchNormalization(axis=-1)(Z) #-- CHANNEL_AXIS (-1)\n",
        "\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def conv_bn_relu(X, **conv_params):\n",
        "\tZnorm = conv_bn(X, **conv_params)\n",
        "\treturn Activation('relu')(Znorm)\n",
        "\t\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def conv_bn_relu_drop(X, **conv_params):\t\n",
        "\tdropout_rate = conv_params.setdefault(\"dropout_rate\", 0.5)\n",
        "\tA = conv_bn_relu(X, **conv_params)\n",
        "\treturn Dropout(dropout_rate)(A)\n",
        "\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def conv_bn_relu_spadrop(X, **conv_params):\t\n",
        "\tdropout_rate = conv_params.setdefault(\"dropout_rate\", 0.5)\n",
        "\tA = conv_bn_relu(X, **conv_params)\n",
        "\treturn SpatialDropout1D(dropout_rate)(A)\n",
        "\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def conv2d_bn(X, **conv_params):\t\n",
        "\tnbunits = conv_params[\"nbunits\"];\n",
        "\tkernel_size = conv_params[\"kernel_size\"];\n",
        "\n",
        "\tstrides = conv_params.setdefault(\"strides\", 1)\n",
        "\tpadding = conv_params.setdefault(\"padding\", \"same\")\n",
        "\tkernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-6))\n",
        "\tkernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "\n",
        "\tZ = Conv2D(nbunits, kernel_size=kernel_size, \n",
        "\t\t\tstrides = strides, padding=padding,\n",
        "\t\t\tkernel_initializer=kernel_initializer,\n",
        "\t\t\tkernel_regularizer=kernel_regularizer)(X)\n",
        "\n",
        "\treturn BatchNormalization(axis=-1)(Z) #-- CHANNEL_AXIS (-1)\n",
        "\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def conv2d_bn_relu(X, **conv_params):\n",
        "\tZnorm = conv2d_bn(X, **conv_params)\n",
        "\treturn Activation('relu')(Znorm)\n",
        "\t\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def conv2d_bn_relu_drop(X, **conv_params):\t\n",
        "\tdropout_rate = conv_params.setdefault(\"dropout_rate\", 0.5)\n",
        "\tA = conv2d_bn_relu(X, **conv_params)\n",
        "\treturn Dropout(dropout_rate)(A)\n",
        "\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def conv2d_bn_relu_spadrop(X, **conv_params):\t\n",
        "\tdropout_rate = conv_params.setdefault(\"dropout_rate\", 0.5)\n",
        "\tA = conv2d_bn_relu(X, **conv_params)\n",
        "\treturn SpatialDropout1D(dropout_rate)(A)\n",
        "\n",
        "\t\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def relu_drop(X, **conv_params):\t\n",
        "\tdropout_rate = conv_params.setdefault(\"dropout_rate\", 0.5)\n",
        "\tA = Activation('relu')(X)\n",
        "\treturn Dropout(dropout_rate)(A)\n",
        "\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def fc_bn(X, **fc_params):\n",
        "\tnbunits = fc_params[\"nbunits\"];\n",
        "\t\n",
        "\tkernel_regularizer = fc_params.setdefault(\"kernel_regularizer\", l2(1.e-6))\n",
        "\tkernel_initializer = fc_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "\t\t\n",
        "\tZ = Dense(nbunits, kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)(X)\n",
        "\treturn BatchNormalization(axis=-1)(Z) #-- CHANNEL_AXIS (-1)\n",
        "\t\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def fc_bn_relu(X, **fc_params):\t\n",
        "\tZnorm = fc_bn(X, **fc_params)\n",
        "\treturn Activation('relu')(Znorm)\n",
        "\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def fc_bn_relu_drop(X, **fc_params):\n",
        "\tdropout_rate = fc_params.setdefault(\"dropout_rate\", 0.5)\n",
        "\tA = fc_bn_relu(X, **fc_params)\n",
        "\treturn Dropout(dropout_rate)(A)\n",
        "\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def softmax(X, nbclasses, **params):\n",
        "\tkernel_regularizer = params.setdefault(\"kernel_regularizer\", l2(1.e-6))\n",
        "\tkernel_initializer = params.setdefault(\"kernel_initializer\", \"glorot_uniform\")\n",
        "\treturn Dense(nbclasses, activation='softmax', \n",
        "\t\t\tkernel_initializer=kernel_initializer,\n",
        "\t\t\tkernel_regularizer=kernel_regularizer)(X)\n",
        "\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def getNoClasses(model_path):\n",
        "\tmodel = load_model(model_path)\n",
        "\tlast_weight = model.get_weights()[-1]\n",
        "\tnclasses = last_weight.shape[0] #--- get size of the bias in the Softmax\n",
        "\treturn nclasses\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "#---------------------- Training models\n",
        "#-----------------------------------------------------------------------\n",
        "#-----------------------------------------------------------------------\n",
        "def trainTestModel(model, X_train, Y_train_onehot, X_test, Y_test_onehot, out_model_file, **train_params):\n",
        "\t#---- variables\n",
        "\tn_epochs = train_params.setdefault(\"n_epochs\", 20)\n",
        "\tbatch_size = train_params.setdefault(\"batch_size\", 32)\n",
        "\t\n",
        "\tlr = train_params.setdefault(\"lr\", 0.001)\n",
        "\tbeta_1 = train_params.setdefault(\"beta_1\", 0.9)\n",
        "\tbeta_2 = train_params.setdefault(\"beta_2\", 0.999)\n",
        "\tdecay = train_params.setdefault(\"decay\", 0.0)\n",
        "\n",
        "\t#---- optimizer\n",
        "\topt = tf.keras.optimizers.Adam(lr=lr, beta_1=beta_1, beta_2=beta_2, decay=decay)\n",
        "\tmodel.compile(optimizer = opt, loss = \"categorical_crossentropy\",\n",
        "\t\t\tmetrics = [\"accuracy\"])\n",
        "\t\n",
        "\t#---- monitoring the minimum loss\n",
        "\tcheckpoint = ModelCheckpoint(out_model_file, monitor='loss',\n",
        "\t\t\tverbose=0, save_best_only=True, mode='min')\n",
        "\tcallback_list = [checkpoint]\n",
        "\t\t\n",
        "\tstart_train_time = time.time()\n",
        "\thist = model.fit(x = X_train, y = Y_train_onehot, epochs = n_epochs, \n",
        "\t\tbatch_size = batch_size, shuffle=True,\n",
        "\t\tvalidation_data=(X_test, Y_test_onehot),\n",
        "\t\tverbose=1, callbacks=callback_list)\n",
        "\ttrain_time = round(time.time()-start_train_time, 2)\n",
        "\t\t\n",
        "\t#-- download the best model\n",
        "\tdel model\t\n",
        "\tmodel = load_model(out_model_file)\n",
        "\tstart_test_time = time.time()\n",
        "\ttest_loss, test_acc = model.evaluate(x=X_test, y=Y_test_onehot, \n",
        "\t\tbatch_size = 128, verbose=0)\n",
        "\ttest_time = round(time.time()-start_test_time, 2)\n",
        "\t\n",
        "\treturn test_acc, np.min(hist.history['loss']), model, hist.history, train_time, test_time\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "def trainTestModel_EarlyAbandon(model, X_train, Y_train_onehot, X_test, Y_test_onehot, out_model_file, **train_params):\n",
        "\t#---- variables\n",
        "\tn_epochs = train_params.setdefault(\"n_epochs\", 20)\n",
        "\tbatch_size = train_params.setdefault(\"batch_size\", 32)\n",
        "\t\n",
        "\tlr = train_params.setdefault(\"lr\", 0.001)\n",
        "\tbeta_1 = train_params.setdefault(\"beta_1\", 0.9)\n",
        "\tbeta_2 = train_params.setdefault(\"beta_2\", 0.999)\n",
        "\tdecay = train_params.setdefault(\"decay\", 0.0)\n",
        "\n",
        "\t#---- optimizer\n",
        "\topt = tf.keras.optimizers.Adam(lr=lr, beta_1=beta_1, beta_2=beta_2, decay=decay)\n",
        "\tmodel.compile(optimizer = opt, loss = \"categorical_crossentropy\",\n",
        "\t\t\tmetrics = [\"accuracy\"])\n",
        "\t\n",
        "\t#---- monitoring the minimum loss\n",
        "\tcheckpoint = ModelCheckpoint(out_model_file, monitor='loss',\n",
        "\t\t\tverbose=0, save_best_only=True, mode='min')\n",
        "\t#early_stop = EarlyStopping(monitor='loss', min_delta=0, patience=0, verbose=0, mode='auto')\n",
        "  #callback_list = [checkpoint, early_stop]\n",
        "\tcallback_list = [checkpoint]\n",
        "\t\t\t\n",
        "\tstart_train_time = time.time()\n",
        "\thist = model.fit(x = X_train, y = Y_train_onehot, epochs = n_epochs, \n",
        "\t\tbatch_size = batch_size, shuffle=True,\n",
        "\t\tvalidation_data=(X_test, Y_test_onehot),\n",
        "\t\tverbose=1, callbacks=callback_list)\n",
        "\ttrain_time = round(time.time()-start_train_time, 2)\n",
        "\t\t\n",
        "\t#-- download the best model\n",
        "\tdel model\t\n",
        "\tmodel = load_model(out_model_file)\n",
        "\tstart_test_time = time.time()\n",
        "\ttest_loss, test_acc = model.evaluate(x=X_test, y=Y_test_onehot, \n",
        "\t\tbatch_size = 128, verbose=0)\n",
        "\ttest_time = round(time.time()-start_test_time, 2)\n",
        "\t\n",
        "\treturn test_acc, np.min(hist.history['loss']), model, hist.history, train_time, test_time\n",
        "\t\t\n",
        "#-----------------------------------------------------------------------\n",
        "def trainValTestModel(model, X_train, Y_train_onehot, X_val, Y_val_onehot, X_test, Y_test_onehot, out_model_file, **train_params):\n",
        "\t#---- variables\n",
        "\tn_epochs = train_params.setdefault(\"n_epochs\", 20)\n",
        "\tbatch_size = train_params.setdefault(\"batch_size\", 32)\n",
        "\t\n",
        "\tlr = train_params.setdefault(\"lr\", 0.001)\n",
        "\tbeta_1 = train_params.setdefault(\"beta_1\", 0.9)\n",
        "\tbeta_2 = train_params.setdefault(\"beta_2\", 0.999)\n",
        "\tdecay = train_params.setdefault(\"decay\", 0.0)\n",
        "\n",
        "\t#---- optimizer\n",
        "\topt = tf.keras.optimizers.Adam(lr=lr, beta_1=beta_1, beta_2=beta_2, decay=decay)\n",
        "\tmodel.compile(optimizer = opt, loss = \"categorical_crossentropy\",\n",
        "\t\t\tmetrics = [\"accuracy\"])\n",
        "\t\n",
        "\t#---- monitoring the minimum validation loss\n",
        "\tcheckpoint = ModelCheckpoint(out_model_file, monitor='val_loss',\n",
        "\t\t\tverbose=0, save_best_only=True, mode='min')\n",
        "\tcallback_list = [checkpoint]\n",
        "\t\t\n",
        "\tstart_train_time = time.time()\n",
        "\thist = model.fit(x = X_train, y = Y_train_onehot, epochs = n_epochs, \n",
        "\t\tbatch_size = batch_size, shuffle=True,\n",
        "\t\tvalidation_data=(X_val, Y_val_onehot),\n",
        "\t\tverbose=1, callbacks=callback_list)\n",
        "\ttrain_time = round(time.time()-start_train_time, 2)\n",
        "\t\t\n",
        "\t#-- download the best model\n",
        "\tdel model\t\n",
        "\tmodel = load_model(out_model_file)\n",
        "\tstart_test_time = time.time()\n",
        "\ttest_loss, test_acc = model.evaluate(x=X_test, y=Y_test_onehot, \n",
        "\t\tbatch_size = 128, verbose=0)\n",
        "\ttest_time = round(time.time()-start_test_time, 2)\n",
        "\t\n",
        "\treturn test_acc, np.min(hist.history['val_loss']), model, hist.history, train_time, test_time\n",
        "\t\n",
        "#-----------------------------------------------------------------------\n",
        "def trainValTestModel_EarlyAbandon(model, X_train, Y_train_onehot, X_val, Y_val_onehot, X_test, Y_test_onehot, out_model_file, **train_params):\n",
        "\t#---- variables\n",
        "\tn_epochs = train_params.setdefault(\"n_epochs\", 20)\n",
        "\tbatch_size = train_params.setdefault(\"batch_size\", 32)\n",
        "\t\n",
        "\tlr = train_params.setdefault(\"lr\", 0.001)\n",
        "\tbeta_1 = train_params.setdefault(\"beta_1\", 0.9)\n",
        "\tbeta_2 = train_params.setdefault(\"beta_2\", 0.999)\n",
        "\tdecay = train_params.setdefault(\"decay\", 0.0)\n",
        "\n",
        "\t#---- optimizer\n",
        "\topt = tf.keras.optimizers.Adam(lr=lr, beta_1=beta_1, beta_2=beta_2, decay=decay)\n",
        "\tmodel.compile(optimizer = opt, loss = \"categorical_crossentropy\",\n",
        "\t\t\tmetrics = [\"accuracy\"])\n",
        "\t\n",
        "\t#---- monitoring the minimum validation loss\n",
        "\tcheckpoint = ModelCheckpoint(out_model_file, monitor='val_loss',\n",
        "\t\t\tverbose=0, save_best_only=True, mode='min')\n",
        "\tearly_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n",
        "\tcallback_list = [checkpoint, early_stop]\n",
        "\t\t\n",
        "\tstart_train_time = time.time()\n",
        "\thist = model.fit(x = X_train, y = Y_train_onehot, epochs = n_epochs, \n",
        "\t\tbatch_size = batch_size, shuffle=True,\n",
        "\t\tvalidation_data=(X_val, Y_val_onehot),\n",
        "\t\tverbose=1, callbacks=callback_list)\n",
        "\ttrain_time = round(time.time()-start_train_time, 2)\n",
        "\t\t\n",
        "\t#-- download the best model\n",
        "\tdel model\t\n",
        "\tmodel = load_model(out_model_file)\n",
        "\tstart_test_time = time.time()\n",
        "\ttest_loss, test_acc = model.evaluate(x=X_test, y=Y_test_onehot, \n",
        "\t\tbatch_size = 128, verbose=0)\n",
        "\ttest_time = round(time.time()-start_test_time, 2)\n",
        "\t\n",
        "\treturn test_acc, np.min(hist.history['val_loss']), model, hist.history, train_time, test_time\n"
      ],
      "metadata": {
        "id": "33e3AXoOuhLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\" \n",
        "\tDefining keras architecture.\n",
        "\t4.4. How big and deep model for our data?\n",
        "\t4.4.1. Width influence or the bias-variance trade-off\n",
        "\"\"\"\n",
        "\n",
        "import sys, os\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.layers import Flatten\n",
        "from keras import backend as K\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "#---------------------- ARCHITECTURES\n",
        "#------------------------------------------------------------------------\t\n",
        "\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def Archi_3CONV16_1FC256(X, nbclasses):\n",
        "\t\n",
        "\t#-- get the input sizes\n",
        "\tm, L, depth = X.shape\n",
        "\tinput_shape = (L,depth)\n",
        "\t\n",
        "\t#-- parameters of the architecture\n",
        "\tl2_rate = 1.e-6\n",
        "\tdropout_rate = 0.5\n",
        "\tnb_conv = 3\n",
        "\tnb_fc= 1\n",
        "\tnbunits_conv = 16 #-- will be double\n",
        "\tnbunits_fc = 256 #-- will be double\n",
        "\t\n",
        "\t# Define the input placeholder.\n",
        "\tX_input = Input(input_shape)\n",
        "\t\t\n",
        "\t#-- nb_conv CONV layers\n",
        "\tX = X_input\n",
        "\tfor add in range(nb_conv):\n",
        "\t\tX = conv_bn_relu_drop(X, nbunits=nbunits_conv, kernel_size=5, kernel_regularizer=l2(l2_rate), dropout_rate=dropout_rate)\n",
        "\t#-- Flatten + \t1 FC layers\n",
        "\tX = Flatten()(X)\n",
        "\tfor add in range(nb_fc):\t\n",
        "\t\tX = fc_bn_relu_drop(X, nbunits=nbunits_fc, kernel_regularizer=l2(l2_rate), dropout_rate=dropout_rate)\n",
        "\t\t\n",
        "\t#-- SOFTMAX layer\n",
        "\tout = softmax(X, nbclasses, kernel_regularizer=l2(l2_rate))\n",
        "\t\t\n",
        "\t# Create model.\n",
        "\treturn Model(inputs = X_input, outputs = out, name='Archi_3CONV16_1FC256')\t\n",
        "\t\n",
        "\t\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def Archi_3CONV32_1FC256(X, nbclasses):\n",
        "\t\n",
        "\t#-- get the input sizes\n",
        "\tm, L, depth = X.shape\n",
        "\tinput_shape = (L,depth)\n",
        "\t\n",
        "\t#-- parameters of the architecture\n",
        "\tl2_rate = 1.e-6\n",
        "\tdropout_rate = 0.5\n",
        "\tnb_conv = 3\n",
        "\tnb_fc= 1\n",
        "\tnbunits_conv = 32 #-- will be double\n",
        "\tnbunits_fc = 256 #-- will be double\n",
        "\t\n",
        "\t# Define the input placeholder.\n",
        "\tX_input = Input(input_shape)\n",
        "\t\t\n",
        "\t#-- nb_conv CONV layers\n",
        "\tX = X_input\n",
        "\tfor add in range(nb_conv):\n",
        "\t\tX = conv_bn_relu_drop(X, nbunits=nbunits_conv, kernel_size=5, kernel_regularizer=l2(l2_rate), dropout_rate=dropout_rate)\n",
        "\t#-- Flatten + \t1 FC layers\n",
        "\tX = Flatten()(X)\n",
        "\tfor add in range(nb_fc):\t\n",
        "\t\tX = fc_bn_relu_drop(X, nbunits=nbunits_fc, kernel_regularizer=l2(l2_rate), dropout_rate=dropout_rate)\n",
        "\t\t\n",
        "\t#-- SOFTMAX layer\n",
        "\tout = softmax(X, nbclasses, kernel_regularizer=l2(l2_rate))\n",
        "\t\t\n",
        "\t# Create model.\n",
        "\treturn Model(inputs = X_input, outputs = out, name='Archi_3CONV32_1FC256')\t\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def Archi_3CONV64_1FC256(X, nbclasses):\n",
        "\t\n",
        "\t#-- get the input sizes\n",
        "\tm, L, depth = X.shape\n",
        "\tinput_shape = (L,depth)\n",
        "\t\n",
        "\t#-- parameters of the architecture\n",
        "\tl2_rate = 1.e-6\n",
        "\tdropout_rate = 0.5\n",
        "\tnb_conv = 3\n",
        "\tnb_fc= 1\n",
        "\tnbunits_conv = 64 #-- will be double\n",
        "\tnbunits_fc = 256 #-- will be double\n",
        "\t\n",
        "\t# Define the input placeholder.\n",
        "\tX_input = Input(input_shape)\n",
        "\t\t\n",
        "\t#-- nb_conv CONV layers\n",
        "\tX = X_input\n",
        "\tfor add in range(nb_conv):\n",
        "\t\tX = conv_bn_relu_drop(X, nbunits=nbunits_conv, kernel_size=5, kernel_regularizer=l2(l2_rate), dropout_rate=dropout_rate)\n",
        "\t#-- Flatten + \t1 FC layers\n",
        "\tX = Flatten()(X)\n",
        "\tfor add in range(nb_fc):\t\n",
        "\t\tX = fc_bn_relu_drop(X, nbunits=nbunits_fc, kernel_regularizer=l2(l2_rate), dropout_rate=dropout_rate)\n",
        "\t\t\n",
        "\t#-- SOFTMAX layer\n",
        "\tout = softmax(X, nbclasses, kernel_regularizer=l2(l2_rate))\n",
        "\t\t\n",
        "\t# Create model.\n",
        "\treturn Model(inputs = X_input, outputs = out, name='Archi_3CONV64_1FC256')\t\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def Archi_3CONV128_1FC256(X, nbclasses):\n",
        "\t\n",
        "\t#-- get the input sizes\n",
        "\tm, L, depth = X.shape\n",
        "\tinput_shape = (L,depth)\n",
        "\t\n",
        "\t#-- parameters of the architecture\n",
        "\tl2_rate = 1.e-6\n",
        "\tdropout_rate = 0.5\n",
        "\tnb_conv = 3\n",
        "\tnb_fc= 1\n",
        "\tnbunits_conv = 128 #-- will be double\n",
        "\tnbunits_fc = 256 #-- will be double\n",
        "\t\n",
        "\t# Define the input placeholder.\n",
        "\tX_input = Input(input_shape)\n",
        "\t\t\n",
        "\t#-- nb_conv CONV layers\n",
        "\tX = X_input\n",
        "\tfor add in range(nb_conv):\n",
        "\t\tX = conv_bn_relu_drop(X, nbunits=nbunits_conv, kernel_size=5, kernel_regularizer=l2(l2_rate), dropout_rate=dropout_rate)\n",
        "\t#-- Flatten + \t1 FC layers\n",
        "\tX = Flatten()(X)\n",
        "\tfor add in range(nb_fc):\t\n",
        "\t\tX = fc_bn_relu_drop(X, nbunits=nbunits_fc, kernel_regularizer=l2(l2_rate), dropout_rate=dropout_rate)\n",
        "\t\t\n",
        "\t#-- SOFTMAX layer\n",
        "\tout = softmax(X, nbclasses, kernel_regularizer=l2(l2_rate))\n",
        "\t\t\n",
        "\t# Create model.\n",
        "\treturn Model(inputs = X_input, outputs = out, name='Archi_3CONV128_1FC256')\t\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def Archi_3CONV256_1FC256(X, nbclasses):\n",
        "\t\n",
        "\t#-- get the input sizes\n",
        "\tm, L, depth = X.shape\n",
        "\tinput_shape = (L,depth)\n",
        "\t\n",
        "\t#-- parameters of the architecture\n",
        "\tl2_rate = 1.e-6\n",
        "\tdropout_rate = 0.5\n",
        "\tnb_conv = 3\n",
        "\tnb_fc= 1\n",
        "\tnbunits_conv = 256 #-- will be double\n",
        "\tnbunits_fc = 256 #-- will be double\n",
        "\t\n",
        "\t# Define the input placeholder.\n",
        "\tX_input = Input(input_shape)\n",
        "\t\t\n",
        "\t#-- nb_conv CONV layers\n",
        "\tX = X_input\n",
        "\tfor add in range(nb_conv):\n",
        "\t\tX = conv_bn_relu_drop(X, nbunits=nbunits_conv, kernel_size=5, kernel_regularizer=l2(l2_rate), dropout_rate=dropout_rate)\n",
        "\t#-- Flatten + \t1 FC layers\n",
        "\tX = Flatten()(X)\n",
        "\tfor add in range(nb_fc):\t\n",
        "\t\tX = fc_bn_relu_drop(X, nbunits=nbunits_fc, kernel_regularizer=l2(l2_rate), dropout_rate=dropout_rate)\n",
        "\t\t\n",
        "\t#-- SOFTMAX layer\n",
        "\tout = softmax(X, nbclasses, kernel_regularizer=l2(l2_rate))\n",
        "\t\t\n",
        "\t# Create model.\n",
        "\treturn Model(inputs = X_input, outputs = out, name='Archi_3CONV256_1FC256')\t\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def Archi_3CONV512_1FC256(X, nbclasses):\n",
        "\t\n",
        "\t#-- get the input sizes\n",
        "\tm, L, depth = X.shape\n",
        "\tinput_shape = (L,depth)\n",
        "\t\n",
        "\t#-- parameters of the architecture\n",
        "\tl2_rate = 1.e-6\n",
        "\tdropout_rate = 0.5\n",
        "\tnb_conv = 3\n",
        "\tnb_fc= 1\n",
        "\tnbunits_conv = 512 #-- will be double\n",
        "\tnbunits_fc = 256 #-- will be double\n",
        "\t\n",
        "\t# Define the input placeholder.\n",
        "\tX_input = Input(input_shape)\n",
        "\t\t\n",
        "\t#-- nb_conv CONV layers\n",
        "\tX = X_input\n",
        "\tfor add in range(nb_conv):\n",
        "\t\tX = conv_bn_relu_drop(X, nbunits=nbunits_conv, kernel_size=5, kernel_regularizer=l2(l2_rate), dropout_rate=dropout_rate)\n",
        "\t#-- Flatten + \t1 FC layers\n",
        "\tX = Flatten()(X)\n",
        "\tfor add in range(nb_fc):\t\n",
        "\t\tX = fc_bn_relu_drop(X, nbunits=nbunits_fc, kernel_regularizer=l2(l2_rate), dropout_rate=dropout_rate)\n",
        "\t\t\n",
        "\t#-- SOFTMAX layer\n",
        "\tout = softmax(X, nbclasses, kernel_regularizer=l2(l2_rate))\n",
        "\t\t\n",
        "\t# Create model.\n",
        "\treturn Model(inputs = X_input, outputs = out, name='Archi_3CONV512_1FC256')\t\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------\t\t\n",
        "def Archi_3CONV1024_1FC256(X, nbclasses):\n",
        "\t\n",
        "\t#-- get the input sizes\n",
        "\tm, L, depth = X.shape\n",
        "\tinput_shape = (L,depth)\n",
        "\t\n",
        "\t#-- parameters of the architecture\n",
        "\tl2_rate = 1.e-6\n",
        "\tdropout_rate = 0.5\n",
        "\tnb_conv = 3\n",
        "\tnb_fc= 1\n",
        "\tnbunits_conv = 1024 #-- will be double\n",
        "\tnbunits_fc = 256 #-- will be double\n",
        "\t\n",
        "\t# Define the input placeholder.\n",
        "\tX_input = Input(input_shape)\n",
        "\t\t\n",
        "\t#-- nb_conv CONV layers\n",
        "\tX = X_input\n",
        "\tfor add in range(nb_conv):\n",
        "\t\tX = conv_bn_relu_drop(X, nbunits=nbunits_conv, kernel_size=5, kernel_regularizer=l2(l2_rate), dropout_rate=dropout_rate)\n",
        "\t#-- Flatten + \t1 FC layers\n",
        "\tX = Flatten()(X)\n",
        "\tfor add in range(nb_fc):\t\n",
        "\t\tX = fc_bn_relu_drop(X, nbunits=nbunits_fc, kernel_regularizer=l2(l2_rate), dropout_rate=dropout_rate)\n",
        "\t\t\n",
        "\t#-- SOFTMAX layer\n",
        "\tout = softmax(X, nbclasses, kernel_regularizer=l2(l2_rate))\n",
        "\t\t\n",
        "\t# Create model.\n",
        "\treturn Model(inputs = X_input, outputs = out, name='Archi_3CONV1024_1FC256')\t\n",
        "\n",
        "\n",
        "#--------------------- Switcher for running the architectures\n",
        "def runArchi(noarchi, *args):\n",
        "\t#---- variables\n",
        "\tn_epochs = 20\n",
        "\tbatch_size = 32\n",
        "\t\n",
        "\tswitcher = {\t\t\n",
        "\t\t0: Archi_3CONV16_1FC256,\n",
        "\t\t1: Archi_3CONV32_1FC256,\n",
        "\t\t2: Archi_3CONV64_1FC256,\n",
        "\t\t3: Archi_3CONV128_1FC256,\n",
        "\t\t3: Archi_3CONV256_1FC256,\n",
        "\t\t4: Archi_3CONV512_1FC256,\n",
        "\t\t5: Archi_3CONV1024_1FC256,\n",
        "\t}\n",
        "\tfunc = switcher.get(noarchi, lambda: 0)\n",
        "\tmodel = func(args[0], args[1].shape[1])\n",
        "\t\n",
        "\tif len(args)==5:\n",
        "\t\treturn trainTestModel_EarlyAbandon(model, *args, n_epochs=n_epochs, batch_size=batch_size)\n",
        "\telif len(args)==7:\n",
        "\t\treturn trainValTestModel_EarlyAbandon(model, *args, n_epochs=n_epochs, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "y8Q2PrzhkVrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#---- Extracting a validation set (if necesary)\n",
        "if val_rate > 0:\n",
        "  X_train, y_train, X_val, y_val = extractValSet(X_train, polygon_ids_train, y_train, val_rate)\n",
        "  #--- Computing the one-hot encoding (recomputing it for train)\n",
        "  y_train_one_hot = to_categorical(y_train, n_classes)\n",
        "  y_val_one_hot = to_categorical(y_val, n_classes)\n",
        "\n",
        "if not os.path.isfile(res_file):\n",
        "  if val_rate==0:\n",
        "    res_mat[0,norun], res_mat[1,norun], model, model_hist, res_mat[2,norun], res_mat[3,norun] = runArchi(noarchi, X_train, y_train_one_hot, X_test, y_test_one_hot, out_model_file)\n",
        "  else:\n",
        "    res_mat[0,norun], res_mat[1,norun], model, model_hist, res_mat[2,norun], res_mat[3,norun] = runArchi(noarchi, X_train, y_train_one_hot, X_val, y_val_one_hot, X_test, y_test_one_hot, out_model_file)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VTjqSL-PkVtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_test_path = \"/content/drive/My\\ Drive/Invasives\\ Research\\ UMN/Remote\\ Sensing\\ Master/Leafy\\ Spurge\\ Demography/temporalCNN-master/example\"\n",
        "\n",
        "results_path = \"/content/drive/My\\ Drive/Invasives\\ Research\\ UMN/Remote\\ Sensing\\ Master/Leafy\\ Spurge\\ Demography/results\"\n",
        "\n",
        "!python3 '/content/drive/My Drive/Invasives Research UMN/Remote Sensing Master/Leafy Spurge Demography/temporalCNN-master/run_archi.py' '--sits_path' {train_test_path} '--res_path' {results_path} '--noarchi' 2\n",
        "\n"
      ],
      "metadata": {
        "id": "6-AgavJbkVxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oo-LtutnkVzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-8hofaqDkV19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SW-DVriekV38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5hUpXl_XkV5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cIMWzslqkV8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_e8-eG8JkV-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "512SjxdlkWAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vd427Le_kWCK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}